---
title: "QCB 408 / 508 -- Notes on Week 4"
author: "Kaiqian Zhang"
date: "`r Sys.Date()`"
header-includes:
  - \usepackage{bm, amsthm, amsmath, bbm, xcolor}
output: 
  pdf_document:
    number_sections: true
    extra_dependencies: ["float"]
---

\providecommand{\E}{\operatorname{E}}
\providecommand{\V}{\operatorname{Var}}
\providecommand{\Cov}{\operatorname{Cov}}
\providecommand{\se}{\operatorname{se}}
\providecommand{\logit}{\operatorname{logit}}
\providecommand{\iid}{\; \stackrel{\text{iid}}{\sim}\;}
\providecommand{\asim}{\; \stackrel{.}{\sim}\;}
\providecommand{\xs}{x_1, x_2, \ldots, x_n}
\providecommand{\Xs}{X_1, X_2, \ldots, X_n}
\providecommand{\bB}{\boldsymbol{B}}
\providecommand{\bb}{\boldsymbol{\beta}}
\providecommand{\bx}{\boldsymbol{x}}
\providecommand{\bX}{\boldsymbol{X}}
\providecommand{\by}{\boldsymbol{y}}
\providecommand{\bY}{\boldsymbol{Y}}
\providecommand{\bz}{\boldsymbol{z}}
\providecommand{\bZ}{\boldsymbol{Z}}
\providecommand{\be}{\boldsymbol{e}}
\providecommand{\bE}{\boldsymbol{E}}
\providecommand{\bs}{\boldsymbol{s}}
\providecommand{\bS}{\boldsymbol{S}}
\providecommand{\bP}{\boldsymbol{P}}
\providecommand{\bI}{\boldsymbol{I}}
\providecommand{\bD}{\boldsymbol{D}}
\providecommand{\bd}{\boldsymbol{d}}
\providecommand{\bW}{\boldsymbol{W}}
\providecommand{\bw}{\boldsymbol{w}}
\providecommand{\bM}{\boldsymbol{M}}
\providecommand{\bPhi}{\boldsymbol{\Phi}}
\providecommand{\bphi}{\boldsymbol{\phi}}
\providecommand{\bN}{\boldsymbol{N}}
\providecommand{\bR}{\boldsymbol{R}}
\providecommand{\bu}{\boldsymbol{u}}
\providecommand{\bU}{\boldsymbol{U}}
\providecommand{\bv}{\boldsymbol{v}}
\providecommand{\bV}{\boldsymbol{V}}
\providecommand{\bO}{\boldsymbol{0}}
\providecommand{\bOmega}{\boldsymbol{\Omega}}
\providecommand{\bLambda}{\boldsymbol{\Lambda}}
\providecommand{\bSig}{\boldsymbol{\Sigma}}
\providecommand{\bSigma}{\boldsymbol{\Sigma}}
\providecommand{\bt}{\boldsymbol{\theta}}
\providecommand{\bT}{\boldsymbol{\Theta}}
\providecommand{\bpi}{\boldsymbol{\pi}}
\providecommand{\argmax}{\text{argmax}}
\providecommand{\KL}{\text{KL}}
\providecommand{\fdr}{\text{FDR}}
\providecommand{\pfdr}{{\rm pFDR}}
\providecommand{\mfdr}{{\rm mFDR}}
\providecommand{\bh}{\hat}
\providecommand{\dd}{\lambda}
\providecommand{\q}{\operatorname{q}}

```{r, message=FALSE, echo=FALSE, cache=FALSE}
# this makes the R output formatted the same as Foundations of Applied Statistics
source("https://raw.githubusercontent.com/jdstorey/fas/master/customization/knitr_options.R")
# load libraries used in this chunk
library(gapminder) 
```


# Summary

Recall the following diagram of "central dogma of statistical inference". Previously,  we have talked about the probability part from population to data. Today we will discuss the inference part from data to population. We will begin statistical inference by introducing maximum likelihood estimation (MLE). Then we will learn about exponential family distributions (EFDs), whose special form has certain mathematical convenience in inference. Finally, We will study what pivotal statistic is and some inference strategies used by Frequentists.  

![](./data/population-data.png){width=50%}

Here is a list of topics for this week:

- Likelihood and maximum likelihood estimation (MLE),

- Exponential family distributions (EFDs),

- Frequentist inference from pivotal statistic.

# Take-away This Week

- A *sufficient statistic* for a parameter $\theta$ is a statistic that, in a certain sense, captures all the information about $\theta$ contained in the sample. A statistic $T(\bm{X})$ is a *sufficient statistic* for $\theta$ if $f(\underline{\bm{x}};\bm{\theta}) = g(T(\underline{\bm{x}});\bm{\theta})h(\underline{\bm{x}})$.

- A *pivotal statistic* is a function of observations and unobservable parameters such that the function's probability distribution does not depend on the unknown parameters. 

- The *MLE* is the value of parameter $\bm{\theta}$ that maximizes the likelihood. We usually work with log-liklihood rather than likelihood in the maximum likelihood estimation. 

    + MLE has three important properties: (1) It is consistent. (2) It has equivariance, i.e., if $\hat{\bm{\theta}}_n$ is the MLE of $\bm{\theta}$, then $g(\hat{\bm{\theta}}_n)$ is the MLE of $g(\bm{\theta})$. (3) We can use Fisher information to approximate the standard error of MLE. 
    
    + We can construct an asymptotic pivotal statistic with MLE by using central limit theorem of MLE.
    
    + Here is a useful table of MLEs, standard errors, and pivotal statistics for three common distributions. 
    
    ![](./data/table.png){width=50%}

- The pdf or pmf of an *exponential family distribution (EFD)* parametrized on the observed scale by $\underline{\bm{\theta}}$ has the following form: \begin{align*}
f(x; \underline{\bm{\theta}}) = h(x)\text{exp}\{\sum_{k=1}^{d}\eta_k(\underline{\bm{\theta}})T_k(x) - A(\bm{\eta})\},
\end{align*} where $\bm{\eta} = (\eta_{1}(\underline{\bm{\theta}}), \eta_{2}(\underline{\bm{\theta}}), \dots, \eta_{d}(\underline{\bm{\theta}}))^T$ and $T_1(x), T_2(x), \dots, T_d(x)$ are sufficient statistics for $\eta_1, \eta_2, \dots, \eta_d$. 

- There are three goals in the frequentist statistical inference: (1) point estimate of $\theta$; (2) confidence interval of $\theta$, which is uncertainty of point estimate; (3) hypothesis test to assess specific value(s) of $\theta$. 

# Likelihood and Maximum Likelihood Estimation

## Likelihood

Given a model with independent and identically distributed random variables 

$$
X_1, X_2, \dots, X_n \overset{\text{iid}}{\sim} F_{\bm{\theta}}
$$

or jointly distributed random variables 

$$
(X_1, X_2, \dots, X_n) \sim F_{\bm{\theta}},
$$

where $F$ is some distribution and $\bm{\theta}$ are parameters of $F$. Note that parameters $\bm{\theta}$ should be informative about what we want to know about the population. Note that there are two levels in a study:

- $x_1, x_2, \dots, x_n$ are observed data from the study. We also write $\underline{\bm{x}}$ as a concise form of observed data. 

- $X_1, X_2, \dots, X_n$ are random variables that model observed data. 

**Definition**: We further assume that we have a pdf or a joint pmf for the above model $f(.;\bm{\theta})$. If we are going to evaluate this on observed data $\underline{\bm{x}}$, we obtain a function of $\bm{\theta}$, 

$$
f(\underline{\bm{x}};\bm{\theta}),
$$
which is also *likelihood* 

$$
L(\bm{\theta};\underline{\bm{x}}).
$$
\textcolor{brown}{Note that likelihood is a function of parameters $\bm{\theta}$ given observed data. }

*Example from Casella & Berger*: (*Negative binomial likelihood*) Let $X$ have a negative binomial distribution with $r=3$ and success probability $p$. If $x=2$ is observed, then the likelihood function is the fifth-degree polynomial on $0\leq p \leq 1$ defined by

\begin{align*}
L(p|2) = P_p(X=2) = {4 \choose 2}p^3 (1-p)^2.
\end{align*}

In general, if $X=x$ is observed, then the likelihood function is the polynomial of degree $3+x$,

\begin{align*}
L(p|x) = {{3+x-1} \choose x}p^3(1-p)^x.
\end{align*}

*Extension from FAS*: **Likelihood Principle** states that if $\bm{x}$ and $\bm{y}$ are two sample points such that $L(\theta|\bm{x})$ is proportional to $L(\theta|\bm{y})$, that is, there exists a constant $c(\bm{x}, \bm{y})$ such that 

\begin{align*}
L(\theta|\bm{x}) = c(\bm{x}, \bm{y})L(\theta|\bm{y}) & \text{for all }\theta,
\end{align*}

then the conclusion drawn from $\bm{x}$ and $\bm{y}$ should be identical.
 
## Log-likelihood

**Definition**: *Log-likelihood* is 

$$
\text{log}L(\bm{\theta};\underline{\bm{x}}) = l(\bm{\theta}; \underline{\bm{x}}).
$$

We usually work with log-likelihood because log-likelihood has a sum form rather than a product form in likelihood. Suppose $X_1, X_2, \dots, X_n \overset{\text{iid}}{\sim} f_{\bm{\theta}}$. Then log-likelihood of the observed data is 

\begin{align*}
l(\bm{\theta}; \underline{\bm{x}}) &= \text{log}(L(\bm{\theta;\underline{\bm{x}}}))\\ \notag
&=\text{log}(f(\underline{\bm{x}};\bm{\theta}))\\ \notag
&=\text{log}(\prod_{i=1}^{n}f(\underline{x}_i; \bm{\theta}))\\ \notag
&=\sum_{i=1}^{n}\text{log}f(\underline{x}_i; \bm{\theta})\\ \notag
&= \sum_{i=1}^{n}l(\bm{\theta};\underline{x}_i). \notag 
\end{align*}

## Sufficient statistic

A *sufficient statistic* for a parameter $\theta$ is a statistic that, in a certain sense, captures all the information about $\theta$ contained in the sample.  Any additional information in the sample, besides the value of the sufficient statistic, does not contain any more information about $\theta$. 

**Definition**: A statistic $T(\bm{X})$ is a *sufficient statistic* for $\theta$ if the conditional distribution of the sample $\underline{\bm{x}}$ given the value of $T(\underline{\bm{x}})$ does not depend on $\theta$. In other words, if $f(\underline{\bm{x}};\bm{\theta}) = g(T(\underline{\bm{x}});\bm{\theta})h(\underline{\bm{x}})$, then $T(\underline{\bm{x}})$ is sufficient. \textcolor{brown}{Note that for the likelihood, \begin{align*}
L(\bm{\theta};\underline{\bm{x}}) = g(T(\underline{\bm{x}});\bm{\theta})h(\underline{\bm{x}}) \propto L(\bm{\theta};T(\underline{\bm{x}})). \end{align*}}

*Example*: (*Normal sufficient statistic*) Let $X_1, X_2, \dots, X_n \overset{\text{iid}}{\sim} \mathcal{N}(\mu, \sigma^2)$. We will show that $\overline{X}_n = \frac{1}{n}\sum_{i=1}^{n}X_i$ is a sufficient statistic for $\mu$. Hint: $\sum_{i=1}^{n} (x_i-\mu)^2 = \sum_{i=1}^{n}(x_i - \bar{x})^2 + n(\bar{x}-\mu)^2$.

**\textcolor{olive}{Proof}**: The joint pdf of the sample $\underline{\bm{x}}$ is \begin{align*}
f(\underline{\bm{x}}|\mu) &= \prod_{i=1}^{n} (2\pi \sigma^2)^{-1/2} \text{exp}(-(x_i)-\mu)^2/(2\sigma^2)\\
                          &= (2\pi \sigma^2)^{-n/2}\text{exp}\left( - \sum_{i=1}^{n}(x_i-\mu)^2/(2\sigma^2)\right)\\
                          &= (2\pi \sigma^2)^{-n/2}\text{exp}\left( - \sum_{i=1}^{n}(x_i-\bar{x}+\bar{x}-\mu)^2/(2\sigma^2)\right) & (\text{add and subtract } \bar{x}) \\
                          &= (2\pi \sigma^2)^{-n/2}\text{exp}\left( -\left(\sum_{i=1}^{n}(x_i-\bar{x})^2 + n(\bar{x}-\mu)^2 + 2\sum_{i=1}^{n} (x_i-\bar{x})(\bar{x}-\mu)\right)/(2\sigma^2)\right)\\
                          &= (2\pi \sigma^2)^{-n/2}\text{exp}\left( -\left(\sum_{i=1}^{n}(x_i-\bar{x})^2 + n(\bar{x}-\mu)^2 \right)/(2\sigma^2)\right) &  \text{(since } \sum_{i=1}^{n} x_i-\bar{x} =0).
\end{align*}
Recall that the sample mean $\overline{X}_n$ follows a $\mathcal{N}(\mu, \sigma^2/n)$ distribution. Thus, the ratio of pdfs is \begin{align*}
\frac{f(\underline{\bm{x}}|\mu)}{g(T(\underline{\bm{x}})=\overline{X}_n|\mu)} &= \frac{(2\pi \sigma^2)^{-n/2}\text{exp}\left( -\left(\sum_{i=1}^{n}(x_i-\bar{x})^2 + n(\bar{x}-\mu)^2 \right)/(2\sigma^2)\right)}{(2\pi\sigma^2)^{-1/2}\text{exp}(-n(\bar{x}-\mu)^2/(2\sigma^2))}\\
&= n^{-1/2}(2\pi\sigma^2)^{-(n-1)/2}\text{exp}\left(-\sum_{i=1}^{n}(x_i-\bar{x})^2/(2\sigma^2)\right),
\end{align*}
which does not depend on $\mu$. Therefore the sample mean is a sufficient statistic for $\mu$.

*Example from Casella & Berger*: (*Binomial sufficient statistic*) Let $X_1, \dots, X_n$ be iid Bernoulli random variables with parameters $\theta$, $0<\theta<1$. We will show that $T(\bm{X})=X_1+\dots+X_n$ is a sufficient statistic for $\theta$.

**\textcolor{olive}{Proof}**: Note that $T(\bm{X})$ counts the number of $X_i$s that equal 1, so $T(\bm{X})$ has a Binomial$(n, \theta)$ distribution. The ratio of pmfs is thus

\begin{align*}
\frac{p(\bm{x}|\theta)}{g(T(\bm{x})|\theta)} &= \frac{\prod \theta^{x_i}(1-\theta)^{1-x_i}}{{n \choose t}\theta^t(1-\theta)^{n-t}} & (\text{define } t=\sum x_i)\\
&=\frac{\theta^{\sum x_i}(1-\theta)^{\sum (1-x_i)}}{{n \choose t}\theta^t(1-\theta)^{n-t}} & (\prod \theta^{x_i} = \theta^{\sum x_i})\\
&=\frac{\theta^t (1-\theta)^{n-t}}{{n \choose t}\theta^t(1-\theta)^{n-t}}\\
&= \frac{1}{{n \choose t}}\\
&= \frac{1}{{n \choose \sum x_i}}.
\end{align*}

Since this ratio does not depend on $\theta$, $T(\bm{X})$ is a sufficient statistic for $\theta$. The interpretation is this: The total number of 1s in this Bernoulli sample contains all the information about $\theta$ that is in the data. Other features of the data, such as the exact value of $X_3$, contain no additional information. 

### Extensions of sufficient statistic

+ Minimal sufficient statistic: A sufficient statistic is *minimal sufficient* if it can be represented as a function of any other sufficient statistic. In other words, $S(X)$ is minimal sufficient if and only if

    + $S(X)$ is sufficient, and
    
    + if $T(X)$ is sufficient, then there exists a function $f$ such that $S(X) = f(T(X))$.

+ Complete statistic: A statistic $T$ is *complete* if $\E[g(T)] = 0$ for all $\theta$ and some function $g$ implies that $P(g(T) = 0; \theta) = 1$ for all $\theta$. 

+ Ancillary statistic: An *ancillary statistic* is a measure of a sample whose distribution does not depend on the parameters of the model. An ancillary statistic is a pivotal quantity that is also a statistic.

+ Basu's theorem: If $T(X)$ is complete and sufficient (for $\theta \in \Theta$), and $S(X)$ is ancillary, then $S(X)$ and $T(X)$ are independent for all $\theta \in \Theta$.

## Maximum likelihood estimation

**Definition**: The *MLE* is the value of $\bm{\theta}$ that maximizes the likelihood. In math,

\begin{align*}
\hat{\bm{\theta}}_{MLE} &= \text{argmax}_{\bm{\theta}} L(\bm{\theta}; \underline{\bm{x}}) \\
&= \text{argmax}_{\bm{\theta}} l(\bm{\theta}; \underline{\bm{x}}) \\
&= \text{argmax}_{\bm{\theta}} L(\bm{\theta}; T(\underline{\bm{x}})). 
\end{align*}

*Example*: (*Binomial maximum likelihood estimation*) Given that $X \sim \text{Binomial}(n,p)$. We will show the MLE for $p$ is $\frac{x}{n}$.  

**\textcolor{olive}{Proof}**: The likelihood is 

$$
L(p;x) = {n \choose x}p^x (1-p)^{n-x} \propto p^x(1-p)^{n-x}. 
$$

And the log-likelihood is

$$
l(p;x) \propto x\text{log}p + (n-x)\text{log}(1-p).
$$
We take derivative of the log-likelihood $l(p;x)$ with respect to $p$ and set it zero to obtain MLE:

$$
\frac{\partial}{\partial p}l(p;x) = 0 \Rightarrow \hat{p}_{\text{MLE}} = \frac{x}{n}.
$$

```{r}
set.seed(508)
n <- 1000
p <- 0.3
x <- rbinom(1, n, p)
log.likelihood <- dbinom(x, n, prob = seq(0,1, by=0.01), log=TRUE)
df <- data.frame(p=seq(0,1, by=0.01), logLik = log.likelihood)
```

```{r, warning=FALSE, error=FALSE, message=FALSE, fig.align="center", fig.height=3, fig.width=4, fig.pos="H", fig.cap="Binomial Log-likelihood versus Parameter p"}
ggplot(df, aes(x=p, y=logLik))+
  geom_point(color="#B57865", alpha=0.7, size=2) +
  theme_minimal() + 
  xlab("p") +
  ylab("log-likelihood") + 
  geom_vline(xintercept = x/n, color="#B29082", size=1, alpha=0.5) + 
  geom_text(aes(x=x/n, label="\n p = x/n", y=-2500), 
            colour="darkgray", text=element_text(size=2))
```

Here is a visualization of a minimal simulation on Binomial MLE. We notice that the log-likelihood obtains maximum when p is around $\frac{x}{n}$, which is the MLE as we computed above. 

## Pivotal statistic

**Definition**: A *pivotal statistic* is a function of observations and unobservable parameters such that the function's probability distribution does not depend on the unknown parameters. 

*Example*: (*Pivotal statistic for Binomial*) Using the example above, we have a pivotal statistic

$$
\frac{\hat{p}-p}{\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}} \dot\sim \mathcal{N}(0,1)
$$

for large $n$. Suppose we observe that $\hat{p} = 0.32$. We want to know the sampling distribution of $\hat{p}$. So we can consider the pivotal statistic for this purpose. Note that in this pivotal statistic, we observe $\hat{p}$ but $p$ is unknown. The distribution of this pivotal statistic is $\mathcal{N}(0,1)$, which does not involve $p$.  

*Example from FAS*: (*Pivotal statistic for Normal*) Given that $X_1, X_2, \dots, X_n \sim \mathcal{N}(\mu, \sigma^2)$. The pivotal statistic

$$
\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \dot\sim \mathcal{N}(0,1).
$$

\textcolor{brown}{Note that in general, for a random variable $Y$, it is the case that $(Y-\E[Y])/\sqrt{\V(Y)}$ has population mean 0 and variance 1.} 


## Important properties for MLE

### Assumptions

We assume the following conditions: 

+ $X_1, X_2, \dots, X_n \overset{\text{iid}}{\sim} F_{\bm{\theta}}$,

+ $\hat{\bm{\theta}}_n$ is the MLE from data,

+ "regularity conditions" are met.

### Three important properties

+ MLE is "consistent". In other words,

$$
\hat{\bm{\theta}}_n \overset{\text{p}}{\to} \bm{\theta}. 
$$

+ Equivariance: If $\hat{\bm{\theta}}_n$ is the MLE of $\bm{\theta}$, then $g(\hat{\bm{\theta}}_n)$ is the MLE of $g(\bm{\theta})$. 

*Example*: (*Binomial MLE*) Suppose $X\sim \text{Binomial(n,p)}$. The MLE for $p$ is $\hat{p} = \frac{x}{p}$. Then the MLE of $\text{Var}(X) = np(1-p)$ is $n\hat{p}(1-\hat{p})$. 

+ Fisher information is define as followed. 

\begin{align}
I_n(\theta) &= \text{Var}(\frac{\partial }{\partial \bm{\theta}} l(\bm{\theta}; \underline{\bm{x}})) \\
&= \text{Var}(\frac{\partial }{\partial \bm{\theta}} \sum_{i=1}^{n}l(\bm{\theta};x_i))\\
&= \sum_{i=1}^{n} \text{Var}(\frac{\partial }{\partial \bm{\theta}}l(\bm{\theta}; x_i))\\
&= -\mathbb{E}[\frac{\partial^2}{\partial \bm{\theta}^2} l(\bm{\theta}; \underline{\bm{x}})]\\
&= -\sum_{i=1}^{n}\mathbb{E}[\frac{\partial^2 }{\partial \bm{\theta}^2} l(\bm{\theta};x_i)]
\end{align}

Fisher information gives you standard error of MLE. In general, the standard error of an estimator is the standard deviation of the sampling distributions of the estimator. For MLEs, the standard error of $\hat{\bm{\theta}}_n$ is 

$$
\text{se}(\hat{\bm{\theta}}_n) \equiv \sqrt{\text{Var}(\hat{\bm{\theta}}_n)}.
$$

And we can use Fisher information to approximate 

$$
\text{se}(\hat{\bm{\theta}}_n) \approx \frac{1}{\sqrt{I_n(\bm{\theta})}},
$$

where high Fisher information $I_n(\bm{\theta})$ means low standard error. Note that $I_n(\bm{\theta})$ uses true $\bm{\theta}$. We usually use $\hat{\text{se}}(\hat{\bm{\theta}}_n) = \frac{1}{\sqrt{I_n(\hat{\bm{\theta}})}}$ as the standard error estimator of an MLE.

*Example*: (*Binomial Fisher information*) Suppose $X\sim \text{Binomial}(n,p)$. Then $I_n(p) = \frac{n}{p(1-p)}$.

\textcolor{olive}{Proof}: The log-likelihood is

\begin{align*}
l(n,p;x) = \text{log}{n \choose x} + x\text{log}p + (n-x)\text{log}(1-p).
\end{align*}

We compute the first and second derivative of log-likelihood with respect to $p$.

\begin{align*}
\frac{\partial }{\partial p}l(n,p;x) &= xp^{-1} + (n-x)(p-1)^{-1}\\
\frac{\partial^2 }{\partial p^2}l(n,p;x) &= -xp^{-2} + (x-n)(p-1)^{-2}.
\end{align*}

Note that 

\begin{align*}
\mathbb{I}(p) &= -\mathbb{E}[\frac{\partial^2 }{\partial p^2}l(n,p;x)]\\
&= -\left( -p^{-2}\mathbb{E}[x] + (p-1)^{-2}\mathbb{E}[x-n]\right)\\
&=-\left(-\frac{np}{p^2}+\frac{np-n}{(p-1)^2}\right)  & (\text{since } \mathbb{E}[x]=np)\\
&=\frac{n}{p(1-p)}.
\end{align*}

## Central limit theorem for MLE

**Theorem**: Let $\hat{\bm{\theta}}_n$ be the MLE of $\bm{\theta}$. As $n \to \infty$, 

$$
\frac{\hat{\bm{\theta}}_n - \bm{\theta}}{\text{se}(\hat{\bm{\theta}}_n)} \overset{D}{\to} \mathcal{N}(0,1), 
$$

$$
\frac{\hat{\bm{\theta}}_n - \bm{\theta}}{\hat{\text{se}}(\hat{\bm{\theta}}_n)} \overset{D}{\to} \mathcal{N}(0,1).
$$

### Asymptotic pivotal statistic

We define 

$$
Z = \frac{\hat{\bm{\theta}}_n - \bm{\theta}}{\hat{\text{se}}(\hat{\bm{\theta}}_n)}.
$$
By Central Limit Theorem above, as $n \to \infty$

$$
Z \overset{D}{\to} \mathcal{N}(0,1).
$$
Thus, $Z$ is approximately a pivotal statistic, which we call asymptotic pivotal statistic. 

## Optimality of MLE

The MLE $\hat{\bm{\theta}}_n$ is such that as $n \to \infty$,

$$
\sqrt{n} (\hat{\bm{\theta}}_n - \bm{\theta}) \overset{D}{\to} \mathcal{N}(0,\tau^2).
$$
Suppose $\tilde{\bm{\theta}}_n$ is any other estimator where as $n \to \infty$,

$$
\sqrt{n} (\tilde{\bm{\theta}}_n - \bm{\theta}) \overset{D}{\to} \mathcal{N}(0,\sigma^2).
$$
In follows that $\frac{\tau^2}{\sigma^2}\leq 1$, which suggests that MLE is asymptotically optimal. 

## Delta method

**Definition** *from Casella & Berger*: (*Delta method*) Let $Y_n$ be a sequence of random variables that satisfies $\sqrt{n}(Y_n-\theta) \to \mathcal{N}(0, \sigma^2)$ in distribution. For a given function $g$ and a specific value of $\theta$, suppose that $g'(\theta)$ exists and is not $0$. Then

\begin{align*}
\sqrt{n}[g(Y_n)-g(\theta)] \to \mathcal{N}(0, \sigma^2[g'(\theta)]^2) & \text{  in distribution}. 
\end{align*}

If $g(.)$ is a differentiable function and $g'(\theta) \neq 0$. We have 

$$
g(t) \approx g(\theta) + g'(\theta)(t-\theta).
$$
Suppose I have $\hat{\text{se}}(\hat{\bm{\theta}}_n)$. Using the approximation above, we obtain

$$
\hat{\text{se}}(g(\hat{\bm{\theta}}_n)) = |g'(\hat{\bm{\theta}}_n)| \hat{\text{se}}(\hat{\bm{\theta}}_n).
$$
*Example*: (*Standard error of Binomial variance MLE*)Suppose $X\sim\text{Binomial}(n,p)$. The MLE is $\hat{\theta}_n = \hat{p} = \frac{x}{n}$.  Since $I_n(p) = \frac{n}{p(1-p)}$, $\hat{\text{se}}(\hat{p}) = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$. Our goal here is to compute $\hat{\text{se}}(\hat{p}(1-\hat{p}))$. Let $g(p) = p(1-p)$. Then $g'(p) = 1-2p$. According to the formula above, we obtain

$$
\hat{\text{se}}(\hat{p}(1-\hat{p})) = |1-2\hat{p}|\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.
$$

# Exponential Family Distributions (EFDs)

*Why exponential family distributions?* *Exponential family distributions (EFDs)* provide a generalized parameterization and form of a very large class of distributions used in inference. For example, Binomial, Poisson, Exponential, Normal, Multinomial, MVN, and Dirichlet are all EFDs. The generalized form provides generally applicable formulas for moments, estimators, etc. EFDs also facilitate developing general algorithms for model fitting. (*FAS*)

## Natural single parameter EFD

A simple case of EFD is natural single parameter EFD such that

$$
f(x;\eta) = h(x)\text{exp}\{\eta x - A(\eta)\},
$$
where $\eta$ is a paramter, $h(.)$ is a function only with respect to $x$, and $A(.)$ is a "log-normalizer" to make sure pmf/pdf integrated to 1.

*Example*: (*Binomial exponential family*) Suppose $X \sim \text{Bernoulli}(p)$. We can write the pmf of $X$ into the above form:

\begin{align*}
f(x;p) &= p^x (1-p)^{1-x}\\
       &= \text{exp}\{x\text{log}p + (1-x)\text{log}(1-p)\}\\
       &= \text{exp}\{\text{log}(\frac{p}{1-p})x + \text{log}(1-p)\},
\end{align*}
where we let
\begin{align*}
&\eta(p) = \text{log}(\frac{p}{1-p})\\
&A(\eta) = \text{log}(1+e^{\eta}).
\end{align*}
Then we obtain
\begin{align*}
f(x;p) = \text{exp}\{\eta x - A(\eta)\}.
\end{align*}

## Generalized EFD

Here is a general definition for EFD. If a random varaible $X$ follows an exponential family distribution parametrized on the observed scale by $\underline{\bm{\theta}}$, then its pdf or pmf has the following form:

\begin{align*}
f(x; \underline{\bm{\theta}}) = h(x)\text{exp}\{\sum_{k=1}^{d}\eta_k(\underline{\bm{\theta}})T_k(x) - A(\bm{\eta})\},
\end{align*}
where 
\begin{align*}
\bm{\eta} &= \begin{pmatrix}
           \eta_{1}(\underline{\bm{\theta}}) \\
           \eta_{2}(\underline{\bm{\theta}}) \\
           \vdots \\
           \eta_{d}(\underline{\bm{\theta}})
         \end{pmatrix},
\end{align*}
and $T_1(x), T_2(x), \dots, T_d(x)$ are sufficient statistics for $\eta_1, \eta_2, \dots, \eta_d$. 
Note that functions $\eta_k(\underline{\bm{\theta}})$ for $k=1,2,\dots, d$ map the usual (observed) to the "natural parameter". $A(\bm{\eta})$ is sometimes called "log-normalizer":

\begin{align*}
A(\bm{\eta}) = \text{log} \int h(x)\text{exp}\{\sum_{k=1}^{d}\eta_k(\underline{\bm{\theta}})T_k(x)\} dx.
\end{align*}

*Example*: (*Normal exponential family*) Suppose $X\sim \mathcal{N}(\mu, \sigma^2)$. We can write its pdf in an EFD form. 

\begin{align*}
f(x;\mu, \sigma^2) &= \frac{1}{\sqrt{2\pi \sigma^2}} \text{exp}\{-\frac{(x-\mu)^2}{2\sigma^2}\}\\
&= \frac{1}{\sqrt{2\pi}} \text{exp}\{\frac{\mu}{\sigma^2}x - \frac{1}{2\sigma^2}x^2 - \text{log}(\sigma) - \frac{\mu^2}{2\sigma^2}\}.
\end{align*}

Then we have 
\begin{align*}
\bm{\eta}(\mu, \sigma^2) 
      = \begin{pmatrix}
           \eta_1(\mu, \sigma^2)  \\
           \eta_2(\mu, \sigma^2)  \\
         \end{pmatrix}
       = \begin{pmatrix}
           \frac{\mu}{\sigma^2} \\
           -\frac{1}{2\sigma^2} \\
         \end{pmatrix}   
\end{align*}
\begin{align*}
\bm{T}(x) 
      = \begin{pmatrix}
           T_1(x)  \\
           T_2(x)  \\
         \end{pmatrix}
       = \begin{pmatrix}
           x \\
           x^2 \\
         \end{pmatrix}   
\end{align*}
\begin{align*}
A(\bm{\eta}) = \text{log}(\sigma) + \frac{\mu^2}{2\sigma^2} = -\frac{1}{2}\text{log}(-2\eta_2) - \frac{\eta_1^2}{4\eta_2}.
\end{align*}

## Calculating moments

The first and second moment of $T_k(x)$ are 

\begin{align*}
\frac{\partial}{\partial \eta_k} A(\bm{\eta}) &= \mathbb{E}[T_k(x)]\\
\frac{\partial^2}{\partial \eta_k^2} A(\bm{\eta}) &= \text{Var}[T_k(x)].
\end{align*}

*Example from FAS*: (*Calculate moments with Normal EFD*) Given that $X\sim \mathcal{N}(\mu, \sigma^2)$,

\begin{align*}
\mathbb{E}[X] &= \frac{\partial}{\partial \eta_1}A(\bm{\eta}) = -\frac{\eta_1}{2\eta_2} = \mu,\\
\V(X) & = \frac{\partial^2}{\partial \eta_1^2}A(\bm{\eta}) = - \frac{1}{2\eta_2} = \sigma^2.\\
\end{align*}

## Maximum likelihood of an EFD

Suppose $x_1, x_2, \dots, x_n$ are iid from an EFD. The log-likelihood is 

\begin{align*}
l(\underline{\bm{\eta}}; \underline{\bm{x}}) = \sum_{i=1}^{n}[\text{log}(h(x_i)) + \sum_{k=1}^{d}\eta_k(\underline{\bm{\theta}})T_k(x_i) - A(\underline{\bm{\eta}})].
\end{align*}

We take the derivative of log-likelihood with respect to $\eta_k$ and set it to zero. 

\begin{align*}
\frac{\partial}{\partial \eta_k} l(\underline{\bm{\eta}}; \underline{\bm{x}}) = \sum_{i=1}^{n}T_k(x_i) - n\frac{\partial}{\partial \eta_k}A(\underline{\bm{\eta}}) = 0.
\end{align*}

Thus MLE of $\eta_k$ is the solution to 

\begin{align*}
\frac{1}{n} \sum_{i=1}^{n} T_k(x_i) = \frac{\partial}{\partial \eta_k}A(\underline{\bm{\eta}}) = \mathbb{E}[T_k(x)]. 
\end{align*}

# Frequentist Inference

## Statistical inference

We have observed data that is modeled by a probablity generation process. The probability distribution has parameters informative about the population. Statistical inference reverse engineers this forward process to estimate parameters and provide measures of uncertainty about the estimates such as:

+ parameters: a number that describes a population. A parameter is often a fixed number and we usually do not know its value. 

+ statistic: a number calculated from a sample of data. A statistic is used to estimate a parameter. 

+ sampling distribution: is the probability disribution of the statistic under repeated realizations of the data from the assumed data generating probability distribution. \textcolor{brown}{Note that sampling distribution is how we connect our calculated statistic to the population (i.e. probability model).} 

## Inference goals and strategies

Given data $x_1, x_2, \dots, x_n$ and model $X_1, X_2, \dots, X_n \sim F_{\theta}$. We have the following three goals:

+ point estimate of $\theta$;

+ confidence interval of $\theta$, which is uncertainty of point estimate;

+ hypothesis test to assess specific value(s) of $\theta$.

### Point estimation of $\theta$

*Example*: MLE is a point estimation of $\theta$.

*Example*: Method of moments estimator is a point estimation of $\theta$. 

### Confidence interval of $\theta$

Confidence interval has the form 

$$
(\hat{\theta}-C_l, \hat{\theta}+C_u),
$$
where $C_l, C_u >0$. Here $Pr(\hat{\theta}-C_l \leq \theta \leq \hat{\theta}+C_u;\theta)$ forms the "level" or coverage probability. Note that $\hat{\theta}, C_l, C_u$ are random variables. **Interpretation**: If we repeat the study many times, then the CI $(\hat{\theta}-C_l, \hat{\theta}+C_u)$ will contain the true $\theta$ with a long run frequency equal to $Pr(\hat{\theta}-C_l \leq \theta \leq \hat{\theta}+C_u;\theta)$. 

Let's approximate a $95$% confidence interval for MLEs. 

\begin{align*}
0.95 &\approx Pr(-1.96 \leq \frac{\hat{\theta}-\theta}{\hat{\text{se}}}(\hat{\theta}) \leq 1.96) \\
&= Pr(-1.96 \leq \frac{\theta - \hat{\theta}}{\hat{\text{se}}}(\hat{\theta}) \leq 1.96) \\
&= Pr(-1.96 \cdot \hat{\text{se}}(\hat{\theta}) \leq \theta - \hat{\theta} \leq 1.96 \cdot \hat{\text{se}}(\hat{\theta})) \\
&= Pr(\hat{\theta}-1.96 \cdot \hat{\text{se}}(\hat{\theta}) \leq \theta  \leq \hat{\theta} + 1.96 \cdot \hat{\text{se}}(\hat{\theta})).
\end{align*}

Note that a $(1-\alpha) 100\%$ approximate confidence interval is 

\begin{align*}
(\hat{\theta}-|Z_{\frac{\alpha}{2}}|\hat{\text{se}}(\hat{\theta}), \hat{\theta}+|Z_{\frac{\alpha}{2}}|\hat{\text{se}}(\hat{\theta})),
\end{align*}

where $Z_{\alpha}$ is the $\alpha-$percentile of $\mathcal{N}(0,1)$. 

*Example from FAS*: (*Simulation*)

```{r}
mu <- 5
n <- 20
x <- replicate(10000, rnorm(n=n, mean=mu)) # 10000 studies
m <- apply(x, 2, mean) # the estimate for each study
ci <- cbind(m - 1.96/sqrt(n), m + 1.96/sqrt(n))
head(ci)
cover <- (mu > ci[,1]) & (mu < ci[,2])
mean(cover)
```

*Example*: (*Sample survey*) A sample survey should satisfy the following inequality to control its confidecne interval

\begin{align*}
|Z_{\frac{\alpha}{2}}| \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \leq |Z_{\frac{\alpha}{2}}|\sqrt{\frac{0.5^2}{n}},
\end{align*}

we can solve for $n$ to determine how large we need to sample. 



# References

+ Casella, G., and Berger, R. L. (2002). *Statistical inference*. Duxbury Press.

+ Storey J. (2020). *Foundations of Applied Statistics*. https://jdstorey.org/fas/

# Session Information

```{r}
sessionInfo()
```