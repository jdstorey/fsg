---
title: "QCB 408 / 508 -- Notes on Week 2"
author: "Scott Wolf"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    fig_crop: no
---

\providecommand{\E}{\operatorname{E}}
\providecommand{\V}{\operatorname{Var}}
\providecommand{\Cov}{\operatorname{Cov}}
\providecommand{\se}{\operatorname{se}}
\providecommand{\logit}{\operatorname{logit}}
\providecommand{\iid}{\; \stackrel{\text{iid}}{\sim}\;}
\providecommand{\asim}{\; \stackrel{.}{\sim}\;}
\providecommand{\xs}{x_1, x_2, \ldots, x_n}
\providecommand{\Xs}{X_1, X_2, \ldots, X_n}
\providecommand{\bB}{\boldsymbol{B}}
\providecommand{\bb}{\boldsymbol{\beta}}
\providecommand{\bx}{\boldsymbol{x}}
\providecommand{\bX}{\boldsymbol{X}}
\providecommand{\by}{\boldsymbol{y}}
\providecommand{\bY}{\boldsymbol{Y}}
\providecommand{\bz}{\boldsymbol{z}}
\providecommand{\bZ}{\boldsymbol{Z}}
\providecommand{\be}{\boldsymbol{e}}
\providecommand{\bE}{\boldsymbol{E}}
\providecommand{\bs}{\boldsymbol{s}}
\providecommand{\bS}{\boldsymbol{S}}
\providecommand{\bP}{\boldsymbol{P}}
\providecommand{\bI}{\boldsymbol{I}}
\providecommand{\bD}{\boldsymbol{D}}
\providecommand{\bd}{\boldsymbol{d}}
\providecommand{\bW}{\boldsymbol{W}}
\providecommand{\bw}{\boldsymbol{w}}
\providecommand{\bM}{\boldsymbol{M}}
\providecommand{\bPhi}{\boldsymbol{\Phi}}
\providecommand{\bphi}{\boldsymbol{\phi}}
\providecommand{\bN}{\boldsymbol{N}}
\providecommand{\bR}{\boldsymbol{R}}
\providecommand{\bu}{\boldsymbol{u}}
\providecommand{\bU}{\boldsymbol{U}}
\providecommand{\bv}{\boldsymbol{v}}
\providecommand{\bV}{\boldsymbol{V}}
\providecommand{\bO}{\boldsymbol{0}}
\providecommand{\bOmega}{\boldsymbol{\Omega}}
\providecommand{\bLambda}{\boldsymbol{\Lambda}}
\providecommand{\bSig}{\boldsymbol{\Sigma}}
\providecommand{\bSigma}{\boldsymbol{\Sigma}}
\providecommand{\bt}{\boldsymbol{\theta}}
\providecommand{\bT}{\boldsymbol{\Theta}}
\providecommand{\bpi}{\boldsymbol{\pi}}
\providecommand{\argmax}{\text{argmax}}
\providecommand{\KL}{\text{KL}}
\providecommand{\fdr}{\text{FDR}}
\providecommand{\pfdr}{{\rm pFDR}}
\providecommand{\mfdr}{{\rm mFDR}}
\providecommand{\bh}{\hat}
\providecommand{\dd}{\lambda}
\providecommand{\q}{\operatorname{q}}

```{r, message=FALSE, echo=FALSE, cache=FALSE}
## this makes the R output formatted the same as Foundations of Applied Statistics
source("https://raw.githubusercontent.com/jdstorey/fas/master/customization/knitr_options.R")
## load libraries used in this chunk
library(gapminder) 
library(HardyWeinberg)
```
## Summary


In Week 2 of Foundations of Statistical Genomics, we covered the basic properties of joint, marginal, and conditional distributions and applied them to the example of Hardy-Weinberg equilibrium to see how we can formally model a well known biological model.


- Joint, Marginal, and Conditional Distributions and Their Properties
- Hardy-Weinberg Equilibrium and Non-random Mating
<!-- - Probabilistic models of RNA-seq data -->

## Joint, Marginal, and Conditional Distributions

We know that one can define multiple random variables on the probability space from an experiment. Given this, we must ask how we can explore event spaces spanning multiple random variables -- that is how do we *jointly* specify probabilities involving multiple random variables? To probe this, let's first explore the resulting CDFs.

Let X and Y define random variables on the same probability space then the joint CDF can be written as

\begin{align*}
F_{X,Y}(x,y) &= \Pr((X \leq x, Y \leq y) \\
&= \Pr(\{\omega: X(\omega) \leq x\} \cap \{\omega: Y(\omega) \leq y\})
\end{align*}

This directly extends to $n$ random variables $\Xs$. The joint CDF, $F_{\Xs}$ is

$$F_{\Xs}(\xs) = \Pr(X_1 \leq x_1, \ldots ,X_n \leq x_n)$$.

### PMFs and PDFs

To deal with pmfs and pdfs for multiple random variables, we can directly extend our single random variable definitions. Let's explore, as before, with the bivariate cases first.

#### Discrete Case

<!-- Consider adding the conditional definitions here -->

The joint probability mass function of two discrete random variables, X,Y is given as
\begin{align*}
  f_{X,Y}(x,y) &= \Pr(\{\omega: X(\omega) = x\} \cap \{\omega: Y(\omega) = y\}) \\
  &= \Pr(X = x, Y = y)
\end{align*}

Which can be generalized to $n$ discrete random variables:
$$ f_{\Xs}(\xs) = \Pr(X_1 = x_1, \ldots, X_n = x_n) $$


#### Continuous Case

For the continuous bivariate case we can extend our pdf definition for single distributions to get:

$$f_{X,Y}(x,y) = \frac{\partial^2F_{X,Y}(x,y)}{\partial x \partial y}$$

This definition, just as those above, extends naturally to $n$ variables

$$ f_{\Xs}(\xs) = \frac{\partial^n F_{\Xs}(\xs)}{\partial \xs} $$

### Events in Multivariate Spaces

Following directly from the above definitions we can define the probability of an event $A_{x_1} \times \cdots \times\ A_{x_n} \subseteq \mathbb{R}^n$ as the following generalized formula:

\begin{align*}
\int_{x_1 \in A_{x_1}} \cdots \int_{x_n \in A_{x_n}} dF_{X_n}(x_n) \cdots dF_{X_1}(x_1)
\end{align*}

### Marginal Distributions

In the case of multiple random variables defined on the same probability space, it is natural to ask how we can decompose the joint distribution into individual distributions. This individual distribution within a larger joint space is referred to as the marginal distribution. That is, the marginal distribution gives the distribution of a single variable without reference to other variables. Consider the below probability space where the bottom row and rightmost column represent the marginal distributions for X and Y respectively.

|        | $x_1$    | $x_2$   | $x_3$    | $\bf{f_Y(y)}$ |
|--------|--------|-------|--------|---------|
| $y_1$    | 0.2    | 0.075 | 0.15   | $\bf{0.425}$  |
| $y_2$   | 0.0375 | 0.15  | 0.0375 | $\bf{0.225}$  |
| $y_3$    | 0.15   | 0     | 0.2    | $\bf{0.35}$   |
| $\bf{f_X(x)}$ | $\bf{0.3875}$ | $\bf{0.225}$ | $\bf{0.3875}$ | $\bf{1}$  |

Again using a bivariate distribution with random variables X and Y, the marginal distribution of some random variable, say X, comes from the Law of Total Probability and is given by

\begin{align*}
&f(x) = \sum_{y \in \mathcal{R}_y} f(x, y) & \mbox{(discrete)} \\
&f(x) = \int_{-\infty}^{\infty} f(x, y) dy & \mbox{(continuous)}
\end{align*}

### Independent Random Variables

If two random variables, say X and Y, are independent, then the outcome of one has no effect on the outcome of the other. In our formal notation, that is:


$$ f_{X,Y}(a,b) = f_X(a)f_Y(b)$$

and then factoring of the CDF follows
\begin{align*}
F_{X,Y}(a,b) &= \Pr(X \leq a, Y \leq b) \\
&= \Pr(X \leq a)\Pr(Y \leq b) \\
&= F_X(a)F_Y(b)
\end{align*}


### Conditional Distributions

One of the most important ways for us to deal with multiple random variables defined on the same probability space is to examine how each variable change within subpopulations. Conditional distributions allow us to examine this variation directly.

Note: A good resource for visualizing this can be found at [http://setosa.io/conditional/](http://setosa.io/conditional/).

To dive into this, consider two random variables X and Y. Our conditional random variable $X|Y \sim F_{X|Y}$ comes from the pmf/pdf given by 

$$
f(x | y) = \frac{f(x, y)}{f(y)}. 
$$

Then we have

$$\Pr(X \leq a | Y \leq b) = \frac{\Pr(X \leq a, Y \leq b)}{\Pr(Y \leq b)}$$
which is just 

$$F_{X|Y}(a|Y \leq b) = \frac{F_{X,Y}(a,b)}{F_Y(b)}$$.

Bayes theorem applies to the distributions just as it does to probabilities and gives us:

$$f(x|y) = \frac{f(x,y)}{f(y)}$$

and all of the above notes extend to $n$ random variables.

### Moments and Moment Generating Functions for Joint Distributions

The $k$th conditional moment of a random variable X is

$$\E[X^k | Y =y] = \sum_{x \in R}x^kf(x|y)$$
$$\E[X^k | Y =y] = \int_{-\infty}^{\infty} x^k f(x|y)dx$$

and our variance is
\begin{align*}
\V(X) &= \E[(X-\E[X])^2]\\
&=\E[X^2] -\E[X]^2
\end{align*}

and our covariance, which describes how much the two random variables vary together, is given by

$$\Cov(X,Y) = \E[(X-\E[X])(Y-\E[Y])]$$

which is just modification of the variance formula, $\V(X)=\E[(X-E[X])^2]$.

To get a visual, take look at a figure from [https://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who-understands-only-the-mean](https://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who-understands-only-the-mean).

```{r figurename, echo=FALSE, fig.cap="Figure Visualizing Covariance from Stack Exchange(User: whuber)", out.width = '90%'}
knitr::include_graphics("data/cov.png")
```
We can also use more general definition which is that covariance, $\sigma_{XY}$, is the expected value of a function of X and Y over the space and can be given by

$$\sigma_{XY} = \int \int (x-\E[X])(y-\E[Y])f(x,y)dxdy$$
$$\sum_x \sum_y (x-\E[X])(y-\E[Y])f(x,y)$$

### Linear Transforms on Random Variables

Let X be a random variable then we have:

$$\E\left[a + bX \right] = a + b \E[X]$$
\begin{align*}
E\left[a + bX \right] &= a + b \E[X] \\
&= \int(a + bx)f(x) dx \\
&= a\int f(x)dx + b\int x f(x) dx
a + b \E[X]
\end{align*}
and 
\begin{align*}
\V\left(a + bX \right) &= \E[(bX + a - b\E[X] - a)^2]\\
&= \E[b^2(X-\E[X])^2 \\
&= b^2\E[(X-\E[X])^2] \\
&=b^2 \V(X)
\end{align*}

### Law of Total Variance

The law of total variance, also known as the variance decomposition formula, tells us that if we have two random variables, X and Y, on the same probability space then the variance can be decomposed as follows:

$$\V(X) = \V(\E[X | Y]) + \E[\V(X | Y)]$$

This gives us the proportion of variance explained in the second component and the remaining in the first term.

### Simple Discrete Joint Probability Table Example using R
Note that this example pulls partially from: https://rstudio-pubs-static.s3.amazonaws.com/209289_9f9ba331cccc4e8f8aabdb9273cc76af.html 


For a simplistic example of dealing with discrete joint probability tables in R, see the sample code below.

Consider the probability table below where the columns represent probabilities for three states of Y(say 0,1,2) and the rows represent states(0,1,2 again) from X. Then we have

```{r probability table}
p <- matrix(c(.1,.04,.06,.06,.15,.1,.12,.22,.15), ncol=3, nrow =3, byrow = T)
p
```

Let's calculate marginal distributions from our probability table!
```{r marginal probabilities}
p_x <- apply(p,1,sum) # for x
p_x

p_y <- apply(p,2,sum) # for y
p_y
```
We can also directly calculate conditional probabilities from the table! Say we want to find the probability of $x_3$ given $y_2$:
```{r conditional probability}
p[3,2]/p_y[2]
```
Now that we've done that, it's obvious that we could continue to calculate the expected value and sum by just assigning values to each state and using the formulas we've learned! Of course this example is a bit simpler than what we would normally run across in practice, but it gives a good example how we would actually deal with a table.

## Hardy-Weinberg Equilibrium

The Hardy-Weinberg Theorem gives a framework for understanding Mendelian genetics in the frame of diploid organisms that reproduce sexually. Given the five following assumptions, the theorem tells us that allele frequencies will be in a completely static state, known as Hardy-Weinberg Equilibrium, from generation to generation and that if the allele frequencies are given as p and (1-p) then the expected genotype frequencies are $p^2$, $2p(1-p)$, and $(1-p)^2$.

1. No non-random mating with respect to the locus in question
2. Infinite population size(that is, no genetic drift!)
3. No migration moves genes in or out of the population
4. No mutation occurs at the locus in question
5. Natural selection is not acting on the locus

For example, considering the the case of some allele with two states A and a. Then let the frequency of A be p and a be (1-p) then the Hardy-Weinberg principle tells us that if the above assumptions are met the frequency of AA is $p^2$, the frequency of Aa is $2p(1-p)$, and the frequency of aa is $(1-p)^2$. Many other properties of this have been shown including the expansion to more than two alleles and generalization to polyploidy among others.

Now that we have an idea of Hardy-Weinberg Equlibrium in general, let's dive into a proper statistical description of the phenomenon.

Take some random variable describing the state of some SNP in terms of the count of alternative alleles in diploid organism: $X \in {0,1,2}$. We should also note that p will represent the frequency of the major allele in the following notes.

Then we get $\Pr(X = k) = r_k$ with $k=0,1,2$, and following directly we get $\E[X] = r_1 + 2r_2$ and $\E[\frac{X}{2}] = \frac{r_1}{2} + r_2 = p$.

Then let $Y \in \{ 0,1 \}$ be the transmitted allele coming from an individual.

\begin{equation}
  \Pr(Y = 1 | X = k) =
    \begin{cases}
      0 & \text{if k = 0}\\
      \frac{1}{2} & \text{if k = 1}\\
      1 & \text{if k = 2}
    \end{cases}       
\end{equation}

Which across all genotypes gives 

\begin{align*}
  \Pr(Y=1) &= \sum^2_{k=0}\Pr(Y=1|X=k)\Pr(X=k) \\
  &= 0\cdot r_0 + \frac{1}{2}r_1 + r_2 \\
  &= p
\end{align*}

Which is simply to say that $Y \sim Bernoulli(p)$.

Consider then Z to describe the following generation genotype which is, of course, the sum of $Y_1$ and $Y_2$ which are both described by Bernoulli(p). As mentioned in the first week of class, the sum of Bernoulli's is simply a binomial which gives us 

$$Z \sim Binomial(2,p)$$

Following from the above, we have $\E[Z] = 2p = \E[X]$ which tells us that the transmission probability for the following generation is identical to the prior -- they are in equilibrium. We can also see that the three genotype frequencies given by the Hardy-Weinberg Theorem come direction from the distribution Binomial(2,p).

### Inbreeding

Now that we have dealt with the simplest case of HWE, let's consider what happens when inbreeding occurs. Since inbreeders are related, they are more likely to share alleles than otherwise. This causes an excess of homozygous offspring and a decrease in heterozygosity in the population. 

Firstly, we must note that two alleles are said to be identical by descent  (IBD) if they are copies from a common ancestor without recombination. Take I to be a random variable indicating whether or not two randomly drawn alleles are IBD. Also $Y_1$, $Y_2$, and $Y_3$ are copies from unrelated ancestors.

For $Y_1$ and $Y_2$,

\begin{align*}
I = 0 &\implies Y_1,Y_2 \iid Bernoulli(p) \\
I = 1 &\implies Y_1 = Y_2 \sim Bernoulli(p)
\end{align*}

We also have 
$$I \sim Bernoulli(f)$$
where f is the inbreeding coefficient.

Then $$(Z|I=0) \sim Binomial(2,p)$$
$$(Z|I=1) \sim Bernoulli(p)$$
with
$$Y_1,Y_2,Y_3 \iid Bernoulli(p)$$

Quickly this gives us $Z = (Y_1 + Y_2)(1-I) + 2 Y_3 I$.

Assuming there is no inbreeding, we have I = 0 and

\begin{equation}
  \Pr(Z = k | I = 0) =
    \begin{cases}
      (1-p)^2 & \text{k = 0} \\
      2p(1-p) & \text{k = 1} \\
      p^2 & \text{k = 2}
    \end{cases}       
\end{equation}

and for $I = 1$

\begin{equation}
  \Pr(Z = k | I = 0) =
    \begin{cases}
      (1-p) & \text{k = 0} \\
      0 & \text{k = 1} \\
      p & \text{k = 2}
    \end{cases}       
\end{equation}

That is, $(Z|I=1) \sim 2Bernoulli(p)$.

To see the actual probabilies of Z, we have the following from the Law of Total Probability.

\begin{align*}
\Pr(Z=0) &= \Pr(Z=0 | I=0)\Pr(I=0) + \Pr(Z=0|I=1)\Pr(I=1)\\
&=(1-p)^2)(1-f) + (1-p)f \\
&=(1-p)^2 + p(1-p)f
\end{align*}

\begin{align*}
\Pr(Z=1) &= 2p(1-p)(1-f) \\
&= 2p(1-p)(1-f)
\end{align*}

which is less than 2p(1-p). That is to say, inbreeding reduces heterozygosity -- just as we would suspect!

\begin{align*}
\Pr(z=2) &= p^2(1-f) + pf \\
&= p^2 + p(1-p)f
\end{align*}

Then our expected value comes directly from the above: $\E[Z] = 1 \cdot 2p(1-p)(1-f) + 2  \cdot (p^2 +p(1-p)f)$.


For examining the variance, we can use the Law of Total Variance to get

$$\V(Z) = \E[\V(Z|I)] + \V(\E[Z|I])$$

The conditioned variances come to

$$\V(Z|I=0)=2p(1-p)$$

\begin{align*}
\V(Z|I=1) &= \V(2Y_3) \\
&= 4\V(Y_3) \\
&= 4p(1-p)
\end{align*}

$$\V(Z|I) = 2p(1-p)(1-I) + 4p(1-p)I$$

and then using the expected Value of I, we have

\begin{align*}
\E[\V(Z|I)] &= 2p(1-p)(1-f) + 4p(1-p)f \\
&= 2p(1-p)(1+f)
\end{align*}

We can also see that 

$$Z|I=0 = Y_1 + Y_2$$
and
$$Z|I=1 = 2Y_3$$

Given that they each have equal probabilities p, we have

$$E[Z|I] = 2p$$
no matter the status of I. That is, inbreeding has no effect on the mean and our variance in the expected value is 0. That is, $\V(\E[Z|I]) = 0$.

Now that we have dealt with that, let's consider the variance of Z again. We have shown that $\E[\V(Z|I)] = 2p(1-p)(1+f)$ and $\V(\E[Z|I]) = 0$. Using the Law of Total Variance, we have our final result:


$$\V(Z) = 2p(1-p)(1+f)$$.

### Modeling Drift as a Random Variable

Another way to reach the same conclusion found above is to take Q as a continuous distribution describing the frequencies within a population that is divided by some fixation index, f. Then we get Then also $Z|Q \sim Binomial(2,Q)$ and $Z|Q = q \sim Binomial(2,q)$. Note that p is the ancestral allele frequency, and then just as before, we want to find $\E[Z]$ and $\V(Z)$.

We know that we can model the distribution of Q using the Balding-Nichols model, a reparametarization of the Beta distribution given by

$$BN(p,f) = Beta(\frac{1-f}{f}p,\frac{1-f}{f}(1-p))$$
We should also note that the mean of BN(p,f) is p and the variance is fp(1 â€“ p).


Then directly we get $\E[Z] = \E[\E[Z|Q]] = \E[2Q] = 2p$.

Just as in class, let's start with $\Pr(Z = 2)$. For the third step of this, we need to note again that $\V(Q) = \E[Q^2] - \E[Q]^2$.

\begin{align*}
\Pr(Z = 2) &= \int \Pr(Z = 2 | Q = q) f(q) dq \\
&= \int q^2 f(q) dq \\
&= E[Q^2] \\
&= Var(Q) + E[Q]^2\\
&= p(1-p)f + p^2
\end{align*}

Next, let's look at $\Pr(Z=1)$

\begin{align*}
\Pr(Z = 1) &= \int \Pr(Z = 1 | Q = q) f(q) dq \\
&= \int 2q(1-q) f(q) dq \\
&= 2 \int q f(q) dq -2 \int q^2 f(q) dq \\
&= 2\E[Q] - 2\E[Q^2] \\
&= 2p - 2(p(1-p)f + p^2) \\
&= 2p(1-p)(1-f)
\end{align*}

and $\Pr(Z=0)$

\begin{align*}
\Pr(Z = 0) &= \int \Pr(Z = 0 | Q = q) f(q) dq \\
&= \int (1-q)^2 f(q) dq \\
&= \int (1-2q + q^2) f(q) dq\\
&= \int f(q) dq - 2\int q f(q) dq + \int q^2 f(q) dq\\
&= 1 - 2\E[Q] + \E[Q^2] \\
&= p(1-p)f + (1-p)^2
\end{align*}

Now that we've dealt with all of that, let's move on to $\V(Z)$!

\begin{align*}
\V(Z) &= \E[\V(Z|Q)] + \V(E[Z|Q]) \\
&= \E[2Q(1-Q)] + \V(2Q) \\
&= E[2Q] - E[2Q^2] + 4p(1-p)f \\
&= 2p - 2E[Q^2] + 4p(1-p)f \\
&= 2p - 2[\V(Q) + \E[Q]^2] + 4p(1-p)f \\
&= 2p - 2p(1-p)f - 2p^2 + 4p(1-p)f \\
&= 2p(1-p) + 2p(1-p)f \\
&= 2p[p(1-p) + p(1-p)f] \\
&= 2p(1-p)(1+f)
\end{align*}




### HWE in R

To get a better idea of how this works in practice, let's generate some data as if it came from a population in Hardy-Weinberg equilibrium (HWE). Note that this example pulls from the [HardyWeinberg](https://cran.r-project.org/web/packages/HardyWeinberg/HardyWeinberg.pdf) from Graffelman et al. The package provides a great framework for exploring HWE.

```{r fig.width = 5,fig.height=5}
#Plotting equations
par(mar = c(0, 0, 5, 0))
curve(x ^ 2,
      col = 'red',
      main = "Expected Genotype Frequency by Allele Frequency for Population in HWE",
      sub = " (red) AA; (green) Aa; (blue) aa")
curve((1 - x) ^ 2, col = 'blue', add = T)
curve(2 * x * (1 - x), col = 'green', add = T)

#As with 100 populations(written as nm number markers in this case) and sample size 200
hwe <- HWData(nm = 100,n=200)
hwe_inbred <- HWData(nm = 100,n=200,f=.5)
```

Now let's make some plots to see how we can visualize HWE.
```{r fig.width = 7,fig.height=4}
par(mfrow = c(1, 2))
par(mar = c(0, 2, 0, 2))
HWTernaryPlot(hwe)
HWTernaryPlot(hwe_inbred)
mtext(
  "(left) Ternary Plot of Genotypes in HWE (right) Ternary Plot of Genotypes with F_ST = 0.5",
  line = -2,
  outer = TRUE,
  cex = 1
)
```

Note that in the same way this is done, we can test populations for HWE and see how different levels of inbreeding(and other features!) affect genotype frequencies.

## Session Information

```{r}
sessionInfo()
```