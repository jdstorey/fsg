<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="John D. Storey" />
  <title>QCB 508 – Week 6</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="libs/reveal.js-3.3.0.1/css/reveal.css"/>



<link rel="stylesheet" href="libs/reveal.js-3.3.0.1/css/theme/simple.css" id="theme">


  <!-- some tweaks to reveal css -->
  <style type="text/css">
    .reveal h1 { font-size: 2.0em; }
    .reveal h2 { font-size: 1.5em;  }
    .reveal h3 { font-size: 1.25em;	}
    .reveal h4 { font-size: 1em;	}

    .reveal .slides>section,
    .reveal .slides>section>section {
      padding: 0px 0px;
    }



    .reveal table {
      border-width: 1px;
      border-spacing: 2px;
      border-style: dotted;
      border-color: gray;
      border-collapse: collapse;
      font-size: 0.7em;
    }

    .reveal table th {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      font-weight: bold;
      border-style: dotted;
      border-color: gray;
    }

    .reveal table td {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      border-style: dotted;
      border-color: gray;
    }


  </style>

    <style type="text/css">code{white-space: pre;}</style>


<!-- Printing and PDF exports -->
<script id="paper-css" type="application/dynamic-css">

/* Default Print Stylesheet Template
   by Rob Glazebrook of CSSnewbie.com
   Last Updated: June 4, 2008

   Feel free (nay, compelled) to edit, append, and
   manipulate this file as you see fit. */


@media print {

	/* SECTION 1: Set default width, margin, float, and
	   background. This prevents elements from extending
	   beyond the edge of the printed page, and prevents
	   unnecessary background images from printing */
	html {
		background: #fff;
		width: auto;
		height: auto;
		overflow: visible;
	}
	body {
		background: #fff;
		font-size: 20pt;
		width: auto;
		height: auto;
		border: 0;
		margin: 0 5%;
		padding: 0;
		overflow: visible;
		float: none !important;
	}

	/* SECTION 2: Remove any elements not needed in print.
	   This would include navigation, ads, sidebars, etc. */
	.nestedarrow,
	.controls,
	.fork-reveal,
	.share-reveal,
	.state-background,
	.reveal .progress,
	.reveal .backgrounds {
		display: none !important;
	}

	/* SECTION 3: Set body font face, size, and color.
	   Consider using a serif font for readability. */
	body, p, td, li, div {
		font-size: 20pt!important;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		color: #000;
	}

	/* SECTION 4: Set heading font face, sizes, and color.
	   Differentiate your headings from your body text.
	   Perhaps use a large sans-serif for distinction. */
	h1,h2,h3,h4,h5,h6 {
		color: #000!important;
		height: auto;
		line-height: normal;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		text-shadow: 0 0 0 #000 !important;
		text-align: left;
		letter-spacing: normal;
	}
	/* Need to reduce the size of the fonts for printing */
	h1 { font-size: 28pt !important;  }
	h2 { font-size: 24pt !important; }
	h3 { font-size: 22pt !important; }
	h4 { font-size: 22pt !important; font-variant: small-caps; }
	h5 { font-size: 21pt !important; }
	h6 { font-size: 20pt !important; font-style: italic; }

	/* SECTION 5: Make hyperlinks more usable.
	   Ensure links are underlined, and consider appending
	   the URL to the end of the link for usability. */
	a:link,
	a:visited {
		color: #000 !important;
		font-weight: bold;
		text-decoration: underline;
	}
	/*
	.reveal a:link:after,
	.reveal a:visited:after {
		content: " (" attr(href) ") ";
		color: #222 !important;
		font-size: 90%;
	}
	*/


	/* SECTION 6: more reveal.js specific additions by @skypanther */
	ul, ol, div, p {
		visibility: visible;
		position: static;
		width: auto;
		height: auto;
		display: block;
		overflow: visible;
		margin: 0;
		text-align: left !important;
	}
	.reveal pre,
	.reveal table {
		margin-left: 0;
		margin-right: 0;
	}
	.reveal pre code {
		padding: 20px;
		border: 1px solid #ddd;
	}
	.reveal blockquote {
		margin: 20px 0;
	}
	.reveal .slides {
		position: static !important;
		width: auto !important;
		height: auto !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 0 !important;
		zoom: 1 !important;

		overflow: visible !important;
		display: block !important;

		text-align: left !important;
		-webkit-perspective: none;
		   -moz-perspective: none;
		    -ms-perspective: none;
		        perspective: none;

		-webkit-perspective-origin: 50% 50%;
		   -moz-perspective-origin: 50% 50%;
		    -ms-perspective-origin: 50% 50%;
		        perspective-origin: 50% 50%;
	}
	.reveal .slides section {
		visibility: visible !important;
		position: static !important;
		width: auto !important;
		height: auto !important;
		display: block !important;
		overflow: visible !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 60px 20px !important;
		z-index: auto !important;

		opacity: 1 !important;

		page-break-after: always !important;

		-webkit-transform-style: flat !important;
		   -moz-transform-style: flat !important;
		    -ms-transform-style: flat !important;
		        transform-style: flat !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;

		-webkit-transition: none !important;
		   -moz-transition: none !important;
		    -ms-transition: none !important;
		        transition: none !important;
	}
	.reveal .slides section.stack {
		padding: 0 !important;
	}
	.reveal section:last-of-type {
		page-break-after: avoid !important;
	}
	.reveal section .fragment {
		opacity: 1 !important;
		visibility: visible !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;
	}
	.reveal section img {
		display: block;
		margin: 15px 0px;
		background: rgba(255,255,255,1);
		border: 1px solid #666;
		box-shadow: none;
	}

	.reveal section small {
		font-size: 0.8em;
	}

}  
</script>


<script id="pdf-css" type="application/dynamic-css">
    
/**
 * This stylesheet is used to print reveal.js
 * presentations to PDF.
 *
 * https://github.com/hakimel/reveal.js#pdf-export
 */

* {
	-webkit-print-color-adjust: exact;
}

body {
	margin: 0 auto !important;
	border: 0;
	padding: 0;
	float: none !important;
	overflow: visible;
}

html {
	width: 100%;
	height: 100%;
	overflow: visible;
}

/* Remove any elements not needed in print. */
.nestedarrow,
.reveal .controls,
.reveal .progress,
.reveal .playback,
.reveal.overview,
.fork-reveal,
.share-reveal,
.state-background {
	display: none !important;
}

h1, h2, h3, h4, h5, h6 {
	text-shadow: 0 0 0 #000 !important;
}

.reveal pre code {
	overflow: hidden !important;
	font-family: Courier, 'Courier New', monospace !important;
}

ul, ol, div, p {
	visibility: visible;
	position: static;
	width: auto;
	height: auto;
	display: block;
	overflow: visible;
	margin: auto;
}
.reveal {
	width: auto !important;
	height: auto !important;
	overflow: hidden !important;
}
.reveal .slides {
	position: static;
	width: 100%;
	height: auto;

	left: auto;
	top: auto;
	margin: 0 !important;
	padding: 0 !important;

	overflow: visible;
	display: block;

	-webkit-perspective: none;
	   -moz-perspective: none;
	    -ms-perspective: none;
	        perspective: none;

	-webkit-perspective-origin: 50% 50%; /* there isn't a none/auto value but 50-50 is the default */
	   -moz-perspective-origin: 50% 50%;
	    -ms-perspective-origin: 50% 50%;
	        perspective-origin: 50% 50%;
}

.reveal .slides section {
	page-break-after: always !important;

	visibility: visible !important;
	position: relative !important;
	display: block !important;
	position: relative !important;

	margin: 0 !important;
	padding: 0 !important;
	box-sizing: border-box !important;
	min-height: 1px;

	opacity: 1 !important;

	-webkit-transform-style: flat !important;
	   -moz-transform-style: flat !important;
	    -ms-transform-style: flat !important;
	        transform-style: flat !important;

	-webkit-transform: none !important;
	   -moz-transform: none !important;
	    -ms-transform: none !important;
	        transform: none !important;
}

.reveal section.stack {
	margin: 0 !important;
	padding: 0 !important;
	page-break-after: avoid !important;
	height: auto !important;
	min-height: auto !important;
}

.reveal img {
	box-shadow: none;
}

.reveal .roll {
	overflow: visible;
	line-height: 1em;
}

/* Slide backgrounds are placed inside of their slide when exporting to PDF */
.reveal section .slide-background {
	display: block !important;
	position: absolute;
	top: 0;
	left: 0;
	width: 100%;
	z-index: -1;
}

/* All elements should be above the slide-background */
.reveal section>* {
	position: relative;
	z-index: 1;
}

/* Display slide speaker notes when 'showNotes' is enabled */
.reveal .speaker-notes-pdf {
	display: block;
	width: 100%;
	max-height: none;
	left: auto;
	top: auto;
	z-index: 100;
}

/* Display slide numbers when 'slideNumber' is enabled */
.reveal .slide-number-pdf {
	display: block;
	position: absolute;
	font-size: 14px;
}

</script>


<script>
var style = document.createElement( 'style' );
style.type = 'text/css';
var style_script_id = window.location.search.match( /print-pdf/gi ) ? 'pdf-css' : 'paper-css';
var style_script = document.getElementById(style_script_id).text;
style.innerHTML = style_script;
document.getElementsByTagName('head')[0].appendChild(style);
</script>

    <link href="libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
</head>
<body>
<style type="text/css">
p { 
  text-align: left; 
  }
.reveal pre code { 
  color: #000000; 
  background-color: rgb(240,240,240);
  font-size: 1.15em;
  border:none; 
  }
.reveal section img { 
  background:none; 
  border:none; 
  box-shadow:none;
  height: 500px;
  }
}
</style>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">QCB 508 – Week 6</h1>
    <h2 class="author">John D. Storey</h2>
    <h3 class="date">Spring 2020</h3>
</section>

<section><section id="section" class="titleslide slide level1"><h1><img src="images/howto.jpg"></img></h1></section></section>
<section><section id="bayesian-estimation" class="titleslide slide level1"><h1>Bayesian Estimation</h1></section><section id="assumptions" class="slide level2">
<h2>Assumptions</h2>
<p>We will assume that <span class="math inline">\((X_1, X_2, \ldots, X_n) | \theta {\; \stackrel{\text{iid}}{\sim}\;}F_{\theta}\)</span> with prior distribution <span class="math inline">\(\theta \sim F_{\tau}\)</span> unless stated otherwise. Shorthand for the former is <span class="math inline">\(\boldsymbol{X} | \theta {\; \stackrel{\text{iid}}{\sim}\;}F_{\theta}\)</span>.</p>
<p>We will write the pdf or pmf of <span class="math inline">\(X\)</span> as <span class="math inline">\(f(x | \theta)\)</span> as opposed to <span class="math inline">\(f(x ; \theta)\)</span> because in the Bayesian framework this actually represents conditional probability.</p>
<p>We will write the pdf or pmf of <span class="math inline">\(\theta\)</span> as <span class="math inline">\(f(\theta)\)</span> or <span class="math inline">\(f(\theta ; \tau)\)</span> or <span class="math inline">\(f(\theta | \tau)\)</span>. Always remember that prior distributions require paramater values, even if we don’t explicitly write them.</p>
</section><section id="posterior-distribution" class="slide level2">
<h2>Posterior Distribution</h2>
<p>The posterior distribution of <span class="math inline">\(\theta | \boldsymbol{X}\)</span> is obtained through Bayes theorem:</p>
<span class="math display">\[\begin{align*}
f(\theta | \boldsymbol{x}) &amp; = \frac{f(\boldsymbol{x} | \theta) f(\theta)}{f(\boldsymbol{x})} = \frac{f(\boldsymbol{x} | \theta) f(\theta)}{\int f(\boldsymbol{x} | \theta^*) f(\theta^*) d\theta^*} \\
&amp; \propto L(\theta ; \boldsymbol{x}) f(\theta)
\end{align*}\]</span>
</section><section id="posterior-expectation" class="slide level2">
<h2>Posterior Expectation</h2>
<p>A very common point estimate of <span class="math inline">\(\theta\)</span> in Bayesian inference is the posterior expected value:</p>
<span class="math display">\[\begin{align*}
\operatorname{E}[\theta | \boldsymbol{x}]  &amp; = \int \theta f(\theta | \boldsymbol{x}) d\theta \\
 &amp; = \frac{\int \theta L(\theta ; \boldsymbol{x}) f(\theta) d\theta}{\int L(\theta ; \boldsymbol{x}) f(\theta) d\theta}
\end{align*}\]</span>
</section><section id="posterior-interval" class="slide level2">
<h2>Posterior Interval</h2>
<p>The Bayesian analog of the frequentist confidence interval is the <span class="math inline">\(1-\alpha\)</span> posterior interval, where <span class="math inline">\(C_{\ell}\)</span> and <span class="math inline">\(C_{u}\)</span> are determined so that:</p>
<p><span class="math display">\[
1-\alpha = \Pr(C_\ell \leq \theta \leq C_u | \boldsymbol{x})
\]</span></p>
</section><section id="maximum-a-posteriori-probability" class="slide level2">
<h2>Maximum <em>A Posteriori</em> Probability</h2>
<p>The maximum <em>a posteriori</em> probability (MAP) is the value (or values) of <span class="math inline">\(\theta\)</span> that maximize the posterior pdf or pmf:</p>
<span class="math display">\[\begin{align*}
\hat{\theta}_{\text{MAP}} &amp; = \operatorname{argmax}_\theta \Pr(\theta | \boldsymbol{x}) \\
 &amp; = \operatorname{argmax}_\theta L(\theta ; \boldsymbol{x}) f(\theta)
\end{align*}\]</span>
<p>This is a frequentist-esque use of the Bayesian framework.</p>
</section><section id="loss-functions" class="slide level2">
<h2>Loss Functions</h2>
<p>Let <span class="math inline">\(\mathcal{L}\left(\theta, \tilde{\theta}\right)\)</span> be a <strong>loss function</strong> for a given estimator <span class="math inline">\(\tilde{\theta}\)</span>. Examples are</p>
<p><span class="math display">\[
\mathcal{L}\left(\theta, \tilde{\theta}\right) = \left(\theta - \tilde{\theta}\right)^2 \mbox{ or } 
\mathcal{L}\left(\theta, \tilde{\theta}\right) = \left|\theta - \tilde{\theta}\right|.
\]</span></p>
<p>Note that, where the expected value is over <span class="math inline">\(f(\boldsymbol{x}; \theta)\)</span>:</p>
<span class="math display">\[\begin{align*}
\operatorname{E}\left[\left(\theta - \tilde{\theta}\right)^2\right] &amp; = \left(\operatorname{E}\left[\tilde{\theta}\right] - \theta\right)^2 + \operatorname{Var}\left(\tilde{\theta}\right) \\
 &amp; = \mbox{bias}^2 + \mbox{variance}
\end{align*}\]</span>
</section><section id="bayes-risk" class="slide level2">
<h2>Bayes Risk</h2>
<p>The <strong>Bayes risk</strong>, <span class="math inline">\(R\left(\theta, \tilde{\theta}\right)\)</span>, is the expected loss with respect to the posterior:</p>
<p><span class="math display">\[
{\operatorname{E}}\left[ \left. \mathcal{L}\left(\theta, \tilde{\theta}\right) \right| \boldsymbol{x} \right]
= \int \mathcal{L}\left(\theta, \tilde{\theta}\right) f(\theta | \boldsymbol{x}) d\theta
\]</span></p>
</section><section id="bayes-estimators" class="slide level2">
<h2>Bayes Estimators</h2>
<p>The <strong>Bayes estimator</strong> minimizes the Bayes risk.</p>
<p>The posterior expectation <span class="math inline">\({\operatorname{E}}\left[ \left. \theta \right| \boldsymbol{x} \right]\)</span> minimizes the Bayes risk of <span class="math inline">\(\mathcal{L}\left(\theta, \tilde{\theta}\right) = \left(\theta - \tilde{\theta}\right)^2\)</span>.</p>
<p>The median of <span class="math inline">\(f(\theta | \boldsymbol{x})\)</span>, calculated by <span class="math inline">\(F^{-1}_{\theta | \boldsymbol{x}}(1/2)\)</span>, minimizes the Bayes risk of <span class="math inline">\(\mathcal{L}\left(\theta, \tilde{\theta}\right) = \left|\theta - \tilde{\theta}\right|\)</span>.</p>
</section></section>
<section><section id="bayesian-classification" class="titleslide slide level1"><h1>Bayesian Classification</h1></section><section id="assumptions-1" class="slide level2">
<h2>Assumptions</h2>
<p>Let <span class="math inline">\((X_1, X_2, \ldots, X_n) | \theta {\; \stackrel{\text{iid}}{\sim}\;}F_\theta\)</span> where <span class="math inline">\(\theta \in \Theta\)</span> and <span class="math inline">\(\theta \sim F_{\tau}\)</span>. Let <span class="math inline">\(\Theta_0, \Theta_1 \subseteq \Theta\)</span> so that <span class="math inline">\(\Theta_0 \cap \Theta_1 = \varnothing\)</span> and <span class="math inline">\(\Theta_0 \cup \Theta_1 = \Theta\)</span>.</p>
<p>Given observed data <span class="math inline">\(\boldsymbol{x}\)</span>, we wish to classify whether <span class="math inline">\(\theta \in \Theta_0\)</span> or <span class="math inline">\(\theta \in \Theta_1\)</span>.</p>
<p>This is the Bayesian analog of hypothesis testing.</p>
</section><section id="prior-probability-on-h" class="slide level2">
<h2>Prior Probability on <em>H</em></h2>
<p>Let <span class="math inline">\(H\)</span> be a rv such that <span class="math inline">\(H=0\)</span> when <span class="math inline">\(\theta \in \Theta_0\)</span> and <span class="math inline">\(H=1\)</span> when <span class="math inline">\(\theta \in \Theta_1\)</span>.</p>
<p>From the prior distribution on <span class="math inline">\(\theta\)</span>, we can calculate</p>
<p><span class="math display">\[
\Pr(H=0) = \int_{\theta \in \Theta_0} f(\theta) d\theta
\]</span></p>
<p>and <span class="math inline">\(\Pr(H=1) = 1-\Pr(H=0)\)</span>.</p>
</section><section id="posterior-probability" class="slide level2">
<h2>Posterior Probability</h2>
<p>Using Bayes theorem, we can also calculate</p>
<span class="math display">\[\begin{align*}
\Pr(H=0 | \boldsymbol{x}) 
&amp; = \frac{f(\boldsymbol{x} | H=0) \Pr(H=0)}{f(\boldsymbol{x})} \\
&amp; = \frac{\int_{\theta \in \Theta_0} f(\boldsymbol{x} | \theta) f(\theta) d\theta}{\int_{\theta \in \Theta} f(\boldsymbol{x} | \theta) f(\theta) d\theta}
\end{align*}\]</span>
<p>where note that <span class="math inline">\(\Pr(H=1 | \boldsymbol{x}) = 1-\Pr(H=0 | \boldsymbol{x})\)</span>.</p>
</section><section id="loss-function" class="slide level2">
<h2>Loss Function</h2>
<p>Let <span class="math inline">\(\mathcal{L}\left(\tilde{H}, H\right)\)</span> be such that</p>
<span class="math display">\[\begin{align*}
\mathcal{L}\left(\tilde{H}=1, H=0 \right) &amp; = c_{I}\\
\mathcal{L}\left(\tilde{H}=0, H=1 \right) &amp; = c_{II}
\end{align*}\]</span>
<p>for some <span class="math inline">\(c_{I}, c_{II} &gt; 0\)</span>.</p>
</section><section id="bayes-risk-1" class="slide level2">
<h2>Bayes Risk</h2>
<p>The Bayes risk, <span class="math inline">\(R\left(\tilde{H}, H\right)\)</span>, is</p>
<span class="math display">\[\begin{align*}
\operatorname{E}\left[ \left. \mathcal{L}\left(\theta, \tilde{\theta}\right) \right| \boldsymbol{x} \right]
&amp; = c_{I} \Pr(\tilde{H}=1, H=0) + c_{II} \Pr(\tilde{H}=0, H=1) \\
&amp; = c_{I} \Pr(\tilde{H}=1 | H=0) \Pr(H=0) \\
&amp; \quad\quad + c_{II} \Pr(\tilde{H}=0 | H=1) \Pr(H=1)
\end{align*}\]</span>
<p>Notice how this balances what frequentists call Type I error and Type II error.</p>
</section><section id="bayes-rule" class="slide level2">
<h2>Bayes Rule</h2>
<p>The estimate <span class="math inline">\(\tilde{H}\)</span> that minimizes <span class="math inline">\(R\left(\tilde{H}, H\right)\)</span> is</p>
<p><span class="math display">\[\tilde{H}=1 \mbox{ when } \Pr(H=1 | \boldsymbol{x}) \geq \frac{c_{I}}{c_{I} + c_{II}}\]</span></p>
<p>and <span class="math inline">\(\tilde{H}=0\)</span> otherwise.</p>
</section></section>
<section><section id="priors" class="titleslide slide level1"><h1>Priors</h1></section><section id="conjugate-priors" class="slide level2">
<h2>Conjugate Priors</h2>
<p>A <strong>conjugate prior</strong> is a prior distribution for a data generating distribution so that the posterior distribution is of the same type as the prior.</p>
<p>Conjugate priors are useful for obtaining stratightforward calculations of the posterior.</p>
<p>There is a systematic method for calculating conjugate priors for exponential family distributions.</p>
</section><section id="example-beta-bernoulli" class="slide level2">
<h2>Example: Beta-Bernoulli</h2>
<p>Suppose <span class="math inline">\(\boldsymbol{X} | \mu {\; \stackrel{\text{iid}}{\sim}\;}\mbox{Bernoulli}(p)\)</span> and suppose that <span class="math inline">\(p \sim \mbox{Beta}(\alpha, \beta)\)</span>.</p>
<span class="math display">\[\begin{align*}
f(p | \boldsymbol{x}) &amp; \propto L(p ; \boldsymbol{x}) f(p) \\
 &amp; = p^{\sum x_i} (1-p)^{\sum (1-x_i)} p^{\alpha - 1} (1-p)^{\beta-1} \\
 &amp; = p^{\alpha - 1 + \sum x_i} (1-p)^{\beta - 1 + \sum (1-x_i)} \\
 &amp; \propto \mbox{Beta}(\alpha + \sum x_i, \beta + \sum (1-x_i))
\end{align*}\]</span>
<p>Therefore, <span class="math display">\[
{\operatorname{E}}[p | \boldsymbol{x}] = \frac{\alpha + \sum x_i}{\alpha + \beta + n}.
\]</span></p>
</section><section id="example-normal-normal" class="slide level2">
<h2>Example: Normal-Normal</h2>
<p>Suppose <span class="math inline">\(\boldsymbol{X} | \mu {\; \stackrel{\text{iid}}{\sim}\;}\mbox{Normal}(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is known, and suppose that <span class="math inline">\(\mu \sim \mbox{Normal}(a, b^2)\)</span>.</p>
<p>Then it can be shown that <span class="math inline">\(\mu | \boldsymbol{x} \sim \mbox{Normal}({\operatorname{E}}[\mu | \boldsymbol{x}], {\operatorname{Var}}(\mu | \boldsymbol{x}))\)</span> where</p>
<p><span class="math display">\[
{\operatorname{E}}[\mu | \boldsymbol{x}] = \frac{b^2}{\frac{\sigma^2}{n} + b^2} \overline{x} + \frac{\frac{\sigma^2}{n}}{\frac{\sigma^2}{n} + b^2} a
\]</span></p>
<p><span class="math display">\[
{\operatorname{Var}}(\mu | \boldsymbol{x}) = \frac{b^2 \frac{\sigma^2}{n}}{\frac{\sigma^2}{n} + b^2}
\]</span></p>
</section><section id="jeffreys-prior" class="slide level2">
<h2>Jeffreys Prior</h2>
<p>If we do inference based on prior <span class="math inline">\(\theta \sim F_{\tau}\)</span> to obtain <span class="math inline">\(f(\theta | \boldsymbol{x}) \propto L(\theta; \boldsymbol{x}) f(\theta)\)</span>, it follows that this inference may <em>not</em> be invariant to transformations of <span class="math inline">\(\theta\)</span>, such as <span class="math inline">\(\eta = g(\theta)\)</span>.</p>
<p>If we utilize a <strong>Jeffreys prior</strong>, which means it is such that</p>
<p><span class="math display">\[f(\theta) \propto \sqrt{I(\theta)}\]</span></p>
<p>then the prior will be invariant to transformations of <span class="math inline">\(\theta\)</span>. We would want to show that <span class="math inline">\(f(\theta) \propto \sqrt{I(\theta)}\)</span> implies <span class="math inline">\(f(\eta) \propto \sqrt{I(\eta)}\)</span>.</p>
</section><section id="examples-jeffreys-priors" class="slide level2">
<h2>Examples: Jeffreys Priors</h2>
<p> </p>
<p>Normal<span class="math inline">\((\mu, \sigma^2)\)</span>, <span class="math inline">\(\sigma^2\)</span> known: <span class="math inline">\(f(\mu) \propto 1\)</span></p>
<p>Normal<span class="math inline">\((\mu, \sigma^2)\)</span>, <span class="math inline">\(\mu\)</span> known: <span class="math inline">\(f(\sigma) \propto \frac{1}{\sigma}\)</span></p>
<p>Poisson<span class="math inline">\((\lambda)\)</span>: <span class="math inline">\(f(\lambda) \propto \frac{1}{\sqrt{\lambda}}\)</span></p>
<p>Bernoulli<span class="math inline">\((p)\)</span>: <span class="math inline">\(f(p) \propto \frac{1}{\sqrt{p(1-p)}}\)</span></p>
</section><section id="improper-prior" class="slide level2">
<h2>Improper Prior</h2>
<p>An <strong>improper prior</strong> is a prior such that <span class="math inline">\(\int f(\theta) d\theta = \infty\)</span>. Nevertheless, sometimes it still may be the case that <span class="math inline">\(f(\theta | \boldsymbol{x}) \propto L(\theta; \boldsymbol{x}) f(\theta)\)</span> yields a probability distribution.</p>
<p>Take for example the case where <span class="math inline">\(\boldsymbol{X} | \mu {\; \stackrel{\text{iid}}{\sim}\;}\mbox{Normal}(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is known, and suppose that <span class="math inline">\(f(\mu) \propto 1\)</span>. Then <span class="math inline">\(\int f(\theta) d\theta = \infty\)</span>, but</p>
<p><span class="math display">\[ f(\theta | \boldsymbol{x}) \propto L(\theta; \boldsymbol{x}) f(\theta) \sim \mbox{Normal}\left(\overline{x}, \sigma^2/n\right)\]</span></p>
<p>which is a proper probability distribution.</p>
</section></section>
<section><section id="empirical-bayes" class="titleslide slide level1"><h1>Empirical Bayes</h1></section><section id="rationale" class="slide level2">
<h2>Rationale</h2>
<p>Under the scenario that <span class="math inline">\(\boldsymbol{X} | \theta {\; \stackrel{\text{iid}}{\sim}\;}F_{\theta}\)</span> with prior distribution <span class="math inline">\(\theta \sim F_{\tau}\)</span>, we have to determine values for <span class="math inline">\(\tau\)</span>.</p>
<p>The <strong>empirical Bayes</strong> approach uses the observed data to estimate the prior parameter(s), <span class="math inline">\(\tau\)</span>.</p>
<p>This is especially useful for high-dimensional data when many parameters are simultaneously drawn from a prior with multiple observations drawn per parameter realization.</p>
</section><section id="approach" class="slide level2">
<h2>Approach</h2>
<p>The usual approach is to first integrate out the parameter to obtain</p>
<p><span class="math display">\[
f(\boldsymbol{x} ; \tau) = \int f(\boldsymbol{x} | \theta) f(\theta ; \tau) d\theta.
\]</span></p>
<p>An estimation method (such as MLE) is then applied to estimate <span class="math inline">\(\tau\)</span>. Then inference proceeds as usual under the assumption that <span class="math inline">\(\theta \sim f(\theta ; \hat{\tau})\)</span>.</p>
</section><section id="example-normal" class="slide level2">
<h2>Example: Normal</h2>
<p>Suppose that <span class="math inline">\(X_i | \mu_i \sim \mbox{Normal}(\mu_i, 1)\)</span> for <span class="math inline">\(i=1, 2, \ldots, n\)</span> where these rv’s are independent. Also suppose that <span class="math inline">\(\mu_i {\; \stackrel{\text{iid}}{\sim}\;}\mbox{Normal}(a, b^2)\)</span>.</p>
<p><span class="math display">\[
f(x_i ; a, b) = \int f(x_i | \mu_i) f(\mu_i; a, b) d\mu_i \sim \mbox{Normal}(a, 1+b^2).
\]</span></p>
<p><span class="math display">\[
\implies \hat{a} = \overline{x}, \ 1+\hat{b}^2 =  \frac{\sum_{k=1}^n (x_k - \overline{x})^2}{n}
\]</span></p>
</section><section class="slide level2">

<span class="math display">\[\begin{align*}
\operatorname{E}[\mu_i | x_i] &amp; = \frac{1}{1+b^2}a + \frac{b^2}{1+b^2}x_i \implies \\
 &amp; \\
\hat{\operatorname{E}}[\mu_i | x_i] &amp; = \frac{1}{1+\hat{b}^2}\hat{a} + \frac{\hat{b}^2}{1+\hat{b}^2}x_i \\
 &amp; = \frac{n}{\sum_{k=1}^n (x_k - \overline{x})^2} \overline{x} + \left(1-\frac{n}{\sum_{k=1}^n (x_k - \overline{x})^2}\right) x_i
\end{align*}\]</span>
</section></section>
<section><section id="numerical-methods-for-likelihood" class="titleslide slide level1"><h1>Numerical Methods for Likelihood</h1></section><section id="challenges" class="slide level2">
<h2>Challenges</h2>
<p>Frequentist model:</p>
<p><span class="math display">\[X_1, X_2, \ldots, X_n {\; \stackrel{\text{iid}}{\sim}\;}F_{{\boldsymbol{\theta}}}\]</span></p>
<p>Bayesian model:</p>
<p><span class="math display">\[X_1, X_2, \ldots, X_n | {\boldsymbol{\theta}}{\; \stackrel{\text{iid}}{\sim}\;}F_{{\boldsymbol{\theta}}} \mbox{ and } {\boldsymbol{\theta}}\sim F_{\boldsymbol{\tau}}\]</span></p>
<p>Sometimes it’s not possible to find formulas for <span class="math inline">\(\hat{{\boldsymbol{\theta}}}_{\text{MLE}}\)</span>, <span class="math inline">\(\hat{{\boldsymbol{\theta}}}_{\text{MAP}}\)</span>, <span class="math inline">\({\operatorname{E}}[{\boldsymbol{\theta}}| {\boldsymbol{x}}]\)</span>, or <span class="math inline">\(f({\boldsymbol{\theta}}| {\boldsymbol{x}})\)</span>. We have to use numerical methods instead.</p>
</section><section id="approaches" class="slide level2">
<h2>Approaches</h2>
<p>Frequently used <em>numerical</em> approaches to likelihood based inference:</p>
<ul>
<li>Expectation-maximization (EM) algorithm</li>
<li>Variational inference</li>
<li>Markov chain Monte Carlo (MCMC)
<ul>
<li>Metropolis sampling</li>
<li>Metropolis-Hastings sampling</li>
<li>Gibbs sampling</li>
</ul></li>
</ul>
</section></section>
<section><section id="latent-variable-models" class="titleslide slide level1"><h1>Latent Variable Models</h1></section><section id="definition" class="slide level2">
<h2>Definition</h2>
<p>Latent variables (or hidden variables) are random variables that are present in the model, but unobserved.</p>
<p>We will denote latent variables by <span class="math inline">\(Z\)</span>, and we will assume <span class="math display">\[(X_1, Z_1), (X_2, Z_2), \ldots, (X_n, Z_n) {\; \stackrel{\text{iid}}{\sim}\;}F_{{\boldsymbol{\theta}}}.\]</span> A realized value of <span class="math inline">\(Z\)</span> is <span class="math inline">\(z\)</span>, <span class="math inline">\({\boldsymbol{Z}}= (Z_1, Z_2, \ldots, Z_n)^T\)</span>, etc.</p>
<p>The EM algorithm and variational inference involve latent variables.</p>
<p>Bayesian models are a special case of latent variable models: the unobserved random parameters are latent variables.</p>
</section><section id="empirical-bayes-revisited" class="slide level2">
<h2>Empirical Bayes Revisited</h2>
<p>In the earlier EB example, we supposed that <span class="math inline">\(X_i | \mu_i \sim \mbox{Normal}(\mu_i, 1)\)</span> for <span class="math inline">\(i=1, 2, \ldots, n\)</span> where these rv’s are independent, and also that <span class="math inline">\(\mu_i {\; \stackrel{\text{iid}}{\sim}\;}\mbox{Normal}(a, b^2)\)</span>.</p>
<p>The unobserved parameters <span class="math inline">\(\mu_1, \mu_2, \ldots, \mu_n\)</span> are latent variables. In this case, <span class="math inline">\({\boldsymbol{\theta}}= (a, b^2)\)</span>.</p>
</section><section id="normal-mixture-model" class="slide level2">
<h2>Normal Mixture Model</h2>
<p>Suppose <span class="math inline">\({X_1, X_2, \ldots, X_n}{\; \stackrel{\text{iid}}{\sim}\;}F_{{\boldsymbol{\theta}}}\)</span> where <span class="math inline">\({\boldsymbol{\theta}}= (\pi_1, \ldots, \pi_K, \mu_1, \ldots, \mu_K, \sigma^2_1, \ldots, \sigma^2_K)\)</span> with pdf</p>
<p><span class="math display">\[
f({\boldsymbol{x}}; {\boldsymbol{\theta}}) = \prod_{i=1}^n \sum_{k=1}^K \pi_k \frac{1}{\sqrt{2\pi\sigma^2_k}} \exp \left\{ -\frac{(x_i - \mu_k)^2}{2 \sigma^2_k} \right\}.
\]</span></p>
<p>The MLEs of the unknown paramaters cannot be found analytically. This is a mixture common model to work with in applications, so we need to be able to estimate the parameters.</p>
</section><section class="slide level2">

<p>There is a latent variable model that produces the same maerginal distribution and likelihood function. Let <span class="math inline">\({\boldsymbol{Z}}_1, {\boldsymbol{Z}}_2, \ldots, {\boldsymbol{Z}}_n {\; \stackrel{\text{iid}}{\sim}\;}\mbox{Multinomial}_K(1, {\boldsymbol{\pi}})\)</span> where <span class="math inline">\({\boldsymbol{\pi}}= (\pi_1, \ldots, \pi_K)\)</span>. Note that <span class="math inline">\(Z_{ik} \in \{0, 1\}\)</span> and <span class="math inline">\(\sum_{k=1}^K Z_{ik} = 1\)</span>. Let <span class="math inline">\([X_i | Z_{ik} = 1] \sim \mbox{Normal}(\mu_k, \sigma^2_k)\)</span>, where <span class="math inline">\(\{X_i | {\boldsymbol{Z}}_i\}_{i=1}^{n}\)</span> are jointly independent.</p>
<p>The joint pdf is</p>
<p><span class="math display">\[
f({\boldsymbol{x}}, {\boldsymbol{z}}; {\boldsymbol{\theta}}) = \prod_{i=1}^n \prod_{k=1}^K  \left[ \pi_k \frac{1}{\sqrt{2\pi\sigma^2_k}} \exp \left\{ -\frac{(x_i - \mu_k)^2}{2 \sigma^2_k} \right\} \right]^{z_{ik}}.
\]</span></p>
</section><section class="slide level2">

<p>Note that</p>
<p><span class="math display">\[
f({\boldsymbol{x}}, {\boldsymbol{z}}; {\boldsymbol{\theta}}) = \prod_{i=1}^n f(x_i, {\boldsymbol{z}}_i; {\boldsymbol{\theta}}).
\]</span> It can be verified that <span class="math inline">\(f({\boldsymbol{x}}; {\boldsymbol{\theta}})\)</span> is the marginal distribution of this latent variable model:</p>
<p><span class="math display">\[
f(x_i ; {\boldsymbol{\theta}}) = \sum_{{\boldsymbol{z}}_i} f(x_i, {\boldsymbol{z}}_i; {\boldsymbol{\theta}}) = \sum_{k=1}^K \pi_k \frac{1}{\sqrt{2\pi\sigma^2_k}} \exp \left\{ -\frac{(x_i - \mu_k)^2}{2 \sigma^2_k} \right\}.
\]</span></p>
</section><section id="bernoulli-mixture-model" class="slide level2">
<h2>Bernoulli Mixture Model</h2>
<p>Suppose <span class="math inline">\({X_1, X_2, \ldots, X_n}{\; \stackrel{\text{iid}}{\sim}\;}F_{{\boldsymbol{\theta}}}\)</span> where <span class="math inline">\({\boldsymbol{\theta}}= (\pi_1, \ldots, \pi_K, p_1, \ldots, p_K)\)</span> with pdf</p>
<p><span class="math display">\[
f({\boldsymbol{x}}; {\boldsymbol{\theta}}) = \prod_{i=1}^n \sum_{k=1}^K \pi_k p_k^{x_i} (1-p_k)^{1-x_i}.
\]</span></p>
<p>As in the Normal mixture model, the MLEs of the unknown paramaters cannot be found analytically.</p>
</section><section class="slide level2">

<p>As before, there is a latent variable model that produces the same maerginal distribution and likelihood function. Let <span class="math inline">\({\boldsymbol{Z}}_1, {\boldsymbol{Z}}_2, \ldots, {\boldsymbol{Z}}_n {\; \stackrel{\text{iid}}{\sim}\;}\mbox{Multinomial}_K(1, {\boldsymbol{\pi}})\)</span> where <span class="math inline">\({\boldsymbol{\pi}}= (\pi_1, \ldots, \pi_K)\)</span>. Note that <span class="math inline">\(Z_{ik} \in \{0, 1\}\)</span> and <span class="math inline">\(\sum_{k=1}^K Z_{ik} = 1\)</span>. Let <span class="math inline">\([X_i | Z_{ik} = 1] \sim \mbox{Bernoulli}(p_k)\)</span>, where <span class="math inline">\(\{X_i | {\boldsymbol{Z}}_i\}_{i=1}^{n}\)</span> are jointly independent.</p>
<p>The joint pdf is</p>
<p><span class="math display">\[
f({\boldsymbol{x}}, {\boldsymbol{z}}; {\boldsymbol{\theta}}) = \prod_{i=1}^n \prod_{k=1}^K  \left[ p_k^{x_i} (1-p_k)^{1-x_i} \right]^{z_{ik}}.
\]</span></p>
</section></section>
<section><section id="em-algorithm" class="titleslide slide level1"><h1>EM Algorithm</h1></section><section id="rationale-1" class="slide level2">
<h2>Rationale</h2>
<p>For any likelihood function, <span class="math inline">\(L({\boldsymbol{\theta}}; {\boldsymbol{x}}) = f({\boldsymbol{x}}; {\boldsymbol{\theta}})\)</span>, there is an abundance of optimization methods that can be used to find the MLE or MAP. However:</p>
<ul>
<li>Optimization methods can be messy to implement</li>
<li>There may be probabilistic structure that we can use to simplify the optimization process and also provide theoretical guarantees on its convergence</li>
<li>Optimization isn’t necessarily the only goal, but one may also be interested in point estimates of the latent variable values</li>
</ul>
</section><section id="requirement" class="slide level2">
<h2>Requirement</h2>
<p>The expectation-maximization (EM) algorithm allows us to calculate MLEs and MAPs when certain geometric properties are satisfied in the probabilistic model.</p>
<p>In order for the EM algorithm to be a practical approach, then we should have a latent variable model <span class="math inline">\(f({\boldsymbol{x}}, {\boldsymbol{z}}; {\boldsymbol{\theta}})\)</span> that is used to do inference on <span class="math inline">\(f({\boldsymbol{x}}; {\boldsymbol{\theta}})\)</span> or <span class="math inline">\(f({\boldsymbol{\theta}}| {\boldsymbol{x}})\)</span>.</p>
<p>Note: Sometimes <span class="math inline">\(({\boldsymbol{x}}, {\boldsymbol{z}})\)</span> is called the <strong>complete data</strong> and <span class="math inline">\({\boldsymbol{x}}\)</span> is called the <strong>observed data</strong> when we are using the EM as a method for dealing with missing data.</p>
</section><section id="the-algorithm" class="slide level2">
<h2>The Algorithm</h2>
<ol type="1">
<li><p>Choose initial value <span class="math inline">\({\boldsymbol{\theta}}^{(0)}\)</span></p></li>
<li><p>Calculate <span class="math inline">\(f({\boldsymbol{z}}| {\boldsymbol{x}}, {\boldsymbol{\theta}}^{(t)})\)</span></p></li>
<li><p>Calculate <span class="math display">\[Q({\boldsymbol{\theta}}, {\boldsymbol{\theta}}^{(t)}) = {\operatorname{E}}_{{\boldsymbol{Z}}|{\boldsymbol{X}}={\boldsymbol{x}}}\left[\log f({\boldsymbol{x}}, {\boldsymbol{Z}}; {\boldsymbol{\theta}}); {\boldsymbol{\theta}}^{(t)}\right]\]</span></p></li>
<li><p>Set <span class="math display">\[{\boldsymbol{\theta}}^{(t+1)} = {\text{argmax}}_{{\boldsymbol{\theta}}} Q({\boldsymbol{\theta}}, {\boldsymbol{\theta}}^{(t)})\]</span></p></li>
<li><p>Iterate until convergence and set <span class="math inline">\(\widehat{{\boldsymbol{\theta}}} = {\boldsymbol{\theta}}^{(\infty)}\)</span></p></li>
</ol>
</section><section id="qboldsymboltheta-boldsymbolthetat" class="slide level2">
<h2><span class="math inline">\(Q({\boldsymbol{\theta}}, {\boldsymbol{\theta}}^{(t)})\)</span></h2>
<p>Continuous <span class="math inline">\({\boldsymbol{Z}}\)</span>:</p>
<p><span class="math display">\[Q({\boldsymbol{\theta}}, {\boldsymbol{\theta}}^{(t)}) = \int \log f({\boldsymbol{x}}, {\boldsymbol{z}}; {\boldsymbol{\theta}}) f({\boldsymbol{z}}| {\boldsymbol{x}}; {\boldsymbol{\theta}}^{(t)}) d{\boldsymbol{z}}\]</span></p>
<p>Discrete <span class="math inline">\({\boldsymbol{Z}}\)</span>:</p>
<p><span class="math display">\[Q({\boldsymbol{\theta}}, {\boldsymbol{\theta}}^{(t)}) = \sum_{{\boldsymbol{z}}} \log f({\boldsymbol{x}}, {\boldsymbol{z}}; {\boldsymbol{\theta}}) f({\boldsymbol{z}}| {\boldsymbol{x}}; {\boldsymbol{\theta}}^{(t)})\]</span></p>
</section><section id="em-for-map" class="slide level2">
<h2>EM for MAP</h2>
<p>If we wish to calculate the MAP we replace <span class="math inline">\(Q({\boldsymbol{\theta}}, {\boldsymbol{\theta}}^{(t)})\)</span> with</p>
<p><span class="math display">\[Q({\boldsymbol{\theta}}, {\boldsymbol{\theta}}^{(t)}) = {\operatorname{E}}_{{\boldsymbol{Z}}|{\boldsymbol{X}}={\boldsymbol{x}}}\left[\log f({\boldsymbol{x}}, {\boldsymbol{Z}}; {\boldsymbol{\theta}}); {\boldsymbol{\theta}}^{(t)}\right] + \log f({\boldsymbol{\theta}})\]</span></p>
<p>where <span class="math inline">\(f({\boldsymbol{\theta}})\)</span> is the prior distribution on <span class="math inline">\({\boldsymbol{\theta}}\)</span>.</p>
</section></section>
<section><section id="em-examples" class="titleslide slide level1"><h1>EM Examples</h1></section><section id="normal-mixture-model-1" class="slide level2">
<h2>Normal Mixture Model</h2>
<p>Returning to the Normal mixture model <a href="#//normal-mixture-model">introduced earlier</a>, we first calculate</p>
<p><span class="math display">\[
\log f({\boldsymbol{x}}, {\boldsymbol{z}}; {\boldsymbol{\theta}}) = \sum_{i=1}^n \sum_{k=1}^K z_{ik} \log \pi_k + z_{ik} \log \phi(x_i; \mu_k, \sigma^2_k)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\phi(x_i; \mu_k, \sigma^2_k) = \frac{1}{\sqrt{2\pi\sigma^2_k}} \exp \left\{ -\frac{(x_i - \mu_k)^2}{2 \sigma^2_k} \right\}.
\]</span></p>
</section><section class="slide level2">

<p>In caculating</p>
<p><span class="math display">\[Q({\boldsymbol{\theta}}, {\boldsymbol{\theta}}^{(t)}) = {\operatorname{E}}_{{\boldsymbol{Z}}|{\boldsymbol{X}}={\boldsymbol{x}}}\left[\log f({\boldsymbol{x}}, {\boldsymbol{Z}}; {\boldsymbol{\theta}}); {\boldsymbol{\theta}}^{(t)}\right]\]</span></p>
<p>we only need to know <span class="math inline">\({\operatorname{E}}_{{\boldsymbol{Z}}|{\boldsymbol{X}}={\boldsymbol{x}}}[Z_{ik} | {\boldsymbol{x}}; {\boldsymbol{\theta}}]\)</span>, which turns out to be</p>
<p><span class="math display">\[
{\operatorname{E}}_{{\boldsymbol{Z}}|{\boldsymbol{X}}={\boldsymbol{x}}}[Z_{ik} | {\boldsymbol{x}}; {\boldsymbol{\theta}}] = \frac{\pi_k \phi(x_i; \mu_k, \sigma^2_k)}{\sum_{j=1}^K \pi_j \phi(x_i; \mu_j, \sigma^2_j)}.
\]</span></p>
</section><section class="slide level2">

<p>Note that we take</p>
<p><span class="math display">\[Q({\boldsymbol{\theta}}, {\boldsymbol{\theta}}^{(t)}) = {\operatorname{E}}_{{\boldsymbol{Z}}|{\boldsymbol{X}}={\boldsymbol{x}}}\left[\log f({\boldsymbol{x}}, {\boldsymbol{Z}}; {\boldsymbol{\theta}}); {\boldsymbol{\theta}}^{(t)}\right]\]</span></p>
<p>so the parameter in <span class="math inline">\(\log f({\boldsymbol{x}}, {\boldsymbol{Z}}; {\boldsymbol{\theta}})\)</span> is a free <span class="math inline">\({\boldsymbol{\theta}}\)</span>, but the paramaters used to take the conditional expectation of <span class="math inline">\({\boldsymbol{Z}}\)</span> are fixed at <span class="math inline">\({\boldsymbol{\theta}}^{(t)}\)</span>. Let’s define</p>
<p><span class="math display">\[
\hat{z}_{ik}^{(t)} = {\operatorname{E}}\left[z_{ik} | {\boldsymbol{x}}; {\boldsymbol{\theta}}^{(t)}\right] = \frac{\pi^{(t)}_k \phi(x_i; \mu^{(t)}_k, \sigma^{2, (t)}_k)}{\sum_{j=1}^K \pi^{(t)}_j \phi(x_i; \mu^{(t)}_j, \sigma^{2, (t)}_j)}.
\]</span></p>
</section><section id="e-step" class="slide level2">
<h2>E-Step</h2>
<p>We calculate</p>
<p><span class="math display">\[Q({\boldsymbol{\theta}}, {\boldsymbol{\theta}}^{(t)}) = {\operatorname{E}}_{{\boldsymbol{Z}}|{\boldsymbol{X}}={\boldsymbol{x}}}\left[\log f({\boldsymbol{x}}, {\boldsymbol{Z}}; {\boldsymbol{\theta}}); {\boldsymbol{\theta}}^{(t)}\right]\]</span> <span class="math display">\[ = \sum_{i=1}^n \sum_{k=1}^K \hat{z}_{ik}^{(t)} \log \pi_k + \hat{z}_{ik}^{(t)} \log \phi(x_i; \mu_k, \sigma^2_k)\]</span></p>
<p>At this point the parameters making up <span class="math inline">\(\hat{z}_{ik}^{(t)}\)</span> are fixed at <span class="math inline">\({\boldsymbol{\theta}}^{(t)}\)</span>.</p>
</section><section id="m-step" class="slide level2">
<h2>M-Step</h2>
<p>We now caculate <span class="math inline">\({\boldsymbol{\theta}}^{(t+1)} = {\text{argmax}}_{{\boldsymbol{\theta}}} Q({\boldsymbol{\theta}}, {\boldsymbol{\theta}}^{(t)}\)</span>, which yields:</p>
<p><span class="math display">\[
\pi_k^{(t+1)} = \frac{\sum_{i=1}^n \hat{z}_{ik}^{(t)}}{n}
\]</span></p>
<p><span class="math display">\[
\mu_k^{(t+1)} = \frac{\sum_{i=1}^n \hat{z}_{ik}^{(t)} x_i}{\sum_{i=1}^n \hat{z}_{ik}^{(t)}}
\]</span></p>
<p><span class="math display">\[
\sigma_k^{2, (t+1)}  = \frac{\sum_{i=1}^n \hat{z}_{ik}^{(t)} \left(x_i - \mu_k^{(t+1)} \right)^2}{\sum_{i=1}^n \hat{z}_{ik}^{(t)}}
\]</span></p>
<p style="font-size: 0.5em;">
Note: You need to use a <a href="http://math.stackexchange.com/questions/421105/maximum-likelihood-estimator-of-parameters-of-multinomial-distribution">Lagrange multiplier</a> to obtain <span class="math inline">\(\{\pi_k^{(t+1)}\}_{k=1}^{K}\)</span>.
</p>
</section><section id="caveat" class="slide level2">
<h2>Caveat</h2>
<p>If we assign one and only one data point to mixture component <span class="math inline">\(k\)</span>, meaning <span class="math inline">\(\mu_k^{(t)} = x_i\)</span> and <span class="math inline">\(\hat{z}_{ik}^{(t)}=1\)</span> for some <span class="math inline">\(k\)</span> and <span class="math inline">\(i\)</span>, then as <span class="math inline">\(\sigma^{2, (t)}_k \rightarrow 0\)</span>, the likelihood goes to <span class="math inline">\(\infty\)</span>.</p>
<p>Therefore, when implementing the EM algorithm for this particular Normal mixture model, we have to be careful to bound all <span class="math inline">\(\sigma^{2, (t)}_k\)</span> away from zero and avoid this scenario.</p>
</section><section id="yeast-gene-expression" class="slide level2">
<h2>Yeast Gene Expression</h2>
<p style="font-size: 0.75em;">
Measured ratios of the nuclear to cytoplasmic fluorescence for a protein-GFP construct that is hypothesized as being nuclear in mitotic cells and largely cytoplasmic in mating cells.
</p>
<p><img src="week06_files/figure-revealjs/unnamed-chunk-2-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="initialize-values" class="slide level2">
<h2>Initialize Values</h2>
<pre class="r"><code>&gt; set.seed(508)
&gt; B &lt;- 100
&gt; p &lt;- rep(0,B)
&gt; mu1 &lt;- rep(0,B)
&gt; mu2 &lt;- rep(0,B)
&gt; s1 &lt;- rep(0,B)
&gt; s2 &lt;- rep(0,B)
&gt; p[1] &lt;- runif(1, min=0.1, max=0.9)
&gt; mu.start &lt;- sample(x, size=2, replace=FALSE)
&gt; mu1[1] &lt;- min(mu.start)
&gt; mu2[1] &lt;- max(mu.start)
&gt; s1[1] &lt;- var(sort(x)[1:60])
&gt; s2[1] &lt;- var(sort(x)[61:120])
&gt; z &lt;- rep(0,120)</code></pre>
</section><section id="run-em-algorithm" class="slide level2">
<h2>Run EM Algorithm</h2>
<pre class="r"><code>&gt; for(i in 2:B) {
+   z &lt;- (p[i-1]*dnorm(x, mean=mu2[i-1], sd=sqrt(s2[i-1])))/
+     (p[i-1]*dnorm(x, mean=mu2[i-1], sd=sqrt(s2[i-1])) + 
+        (1-p[i-1])*dnorm(x, mean=mu1[i-1], sd=sqrt(s1[i-1])))
+   mu1[i] &lt;- sum((1-z)*x)/sum(1-z)
+   mu2[i] &lt;- sum(z*x)/sum(z)
+   s1[i] &lt;- sum((1-z)*(x-mu1[i])^2)/sum(1-z)
+   s2[i] &lt;- sum(z*(x-mu2[i])^2)/sum(z)
+   p[i] &lt;- sum(z)/length(z)
+ }
&gt; 
&gt; tail(cbind(mu1, s1, mu2, s2, p), n=3)
            mu1        s1    mu2       s2         p
 [98,] 2.455325 0.3637967 6.7952 6.058291 0.5340015
 [99,] 2.455325 0.3637967 6.7952 6.058291 0.5340015
[100,] 2.455325 0.3637967 6.7952 6.058291 0.5340015</code></pre>
</section><section id="fitted-mixture-distribution" class="slide level2">
<h2>Fitted Mixture Distribution</h2>
<p><img src="week06_files/figure-revealjs/unnamed-chunk-5-1.png" width="576" style="display: block; margin: auto;" /></p>
</section><section id="bernoulli-mixture-model-1" class="slide level2">
<h2>Bernoulli Mixture Model</h2>
<p>As an exercise, derive the EM algorithm of the Bernoilli mixture model <a href="#//bernoulli-mixture-model">introduced earlier</a>.</p>
<p>Hint: Replace <span class="math inline">\(\phi(x_i; \mu_k, \sigma^2_k)\)</span> with the appropriate Bernoilli pmf.</p>
</section><section id="other-applications-of-em" class="slide level2">
<h2>Other Applications of EM</h2>
<ul>
<li>Dealing with missing data</li>
<li>Multiple imputation of missing data</li>
<li>Truncated observations</li>
<li>Bayesian hyperparameter estimation</li>
<li>Hidden Markov models</li>
</ul>
</section><section id="em-increases-likelihood" class="slide level2">
<h2>EM Increases Likelihood</h2>
<p>Since <span class="math inline">\({\boldsymbol{\theta}}^{(t+1)} = {\text{argmax}}_{{\boldsymbol{\theta}}} Q({\boldsymbol{\theta}}, {\boldsymbol{\theta}}^{(t)})\)</span>, it follows that</p>
<p><span class="math display">\[Q({\boldsymbol{\theta}}^{(t+1)}, {\boldsymbol{\theta}}^{(t)}) \geq Q({\boldsymbol{\theta}}^{(t)}, {\boldsymbol{\theta}}^{(t)}).\]</span></p>
<p>Chapter 43 of <em>Foundations of Applied Statistics</em> details additional mathematics that shows:</p>
<p><span class="math display">\[
\log f({\boldsymbol{x}}; {\boldsymbol{\theta}}^{(t+1)}) \geq \log f({\boldsymbol{x}}; {\boldsymbol{\theta}}^{(t)}).
\]</span></p>
</section></section>
<section><section id="markov-chain-monte-carlo" class="titleslide slide level1"><h1>Markov Chain Monte Carlo</h1></section><section id="motivation" class="slide level2">
<h2>Motivation</h2>
<p>When performing Bayesian inferece, it is often (but not always) possible to calculate</p>
<p><span class="math display">\[f({\boldsymbol{\theta}}| {\boldsymbol{x}}) \propto L({\boldsymbol{\theta}}; {\boldsymbol{x}}) f({\boldsymbol{\theta}})\]</span></p>
<p>but it is typically much more difficult to calculate</p>
<p><span class="math display">\[f({\boldsymbol{\theta}}| {\boldsymbol{x}}) = \frac{L({\boldsymbol{\theta}}; {\boldsymbol{x}}) f({\boldsymbol{\theta}})}{f({\boldsymbol{x}})}.\]</span></p>
<p>Markov chain Monte Carlo is a method for simulating data approximately from <span class="math inline">\(f({\boldsymbol{\theta}}| {\boldsymbol{x}})\)</span> with knowledge of only <span class="math inline">\(L({\boldsymbol{\theta}}; {\boldsymbol{x}}) f({\boldsymbol{\theta}})\)</span>.</p>
</section><section id="note" class="slide level2">
<h2>Note</h2>
<p>MCMC can be used to approximately simulate data from any distribution that is only proportionally characterized, but it is probably most well know for doing so in the context of Bayesian infererence.</p>
<p>We will explain MCMC in the context of Bayesian inference.</p>
</section><section id="big-picture" class="slide level2">
<h2>Big Picture</h2>
<p>We draw a Markov chain of <span class="math inline">\({\boldsymbol{\theta}}\)</span> values so that, in some asymptotic sense, these are equivalent to iid draws from <span class="math inline">\(f({\boldsymbol{\theta}}| {\boldsymbol{x}})\)</span>.</p>
<p>The draws are done competitively so that the next draw of a realization of <span class="math inline">\({\boldsymbol{\theta}}\)</span> depends on the current value.</p>
<p>The Markov chain is set up so that it only depends on <span class="math inline">\(L({\boldsymbol{\theta}}; {\boldsymbol{x}}) f({\boldsymbol{\theta}})\)</span>.</p>
<p><em>A lot</em> of practical decisions need to be made by the user, so utilize MCMC carefully.</p>
</section><section id="metropolis-hastings-algorithm" class="slide level2">
<h2>Metropolis-Hastings Algorithm</h2>
<ol type="1">
<li><p>Initialize <span class="math inline">\({\boldsymbol{\theta}}^{(0)}\)</span></p></li>
<li><p>Generate <span class="math inline">\({\boldsymbol{\theta}}^{*} \sim q({\boldsymbol{\theta}}| {\boldsymbol{\theta}}^{(b)})\)</span> for some pdf or pmf <span class="math inline">\(q(\cdot | \cdot)\)</span></p></li>
<li><p>With probablity <span class="math display">\[A({\boldsymbol{\theta}}^{*}, {\boldsymbol{\theta}}^{(b)}) = \min\left( 1, \frac{L({\boldsymbol{\theta}}^{*}; {\boldsymbol{x}}) f({\boldsymbol{\theta}}^{*}) q({\boldsymbol{\theta}}^{(b)} | {\boldsymbol{\theta}}^{*})}{L({\boldsymbol{\theta}}^{(b)}; {\boldsymbol{x}}) f({\boldsymbol{\theta}}^{(b)}) q({\boldsymbol{\theta}}^{*} | {\boldsymbol{\theta}}^{(b)})} \right)\]</span> set <span class="math inline">\({\boldsymbol{\theta}}^{(b+1)} = {\boldsymbol{\theta}}^{*}\)</span>. Otherise, set <span class="math inline">\({\boldsymbol{\theta}}^{(b+1)} = {\boldsymbol{\theta}}^{(b)}\)</span></p></li>
<li><p>Continue for <span class="math inline">\(b = 1, 2, \ldots, B\)</span> iterations and <em>carefully</em> select which <span class="math inline">\({\boldsymbol{\theta}}^{(b)}\)</span> are utilized to approximate iid observations from <span class="math inline">\(f({\boldsymbol{\theta}}| {\boldsymbol{x}})\)</span></p></li>
</ol>
</section><section id="metropolis-algorithm" class="slide level2">
<h2>Metropolis Algorithm</h2>
<p>The Metropolis algorithm restricts <span class="math inline">\(q(\cdot, \cdot)\)</span> to be symmetric so that <span class="math inline">\(q({\boldsymbol{\theta}}^{(b)} | {\boldsymbol{\theta}}^{*}) = q({\boldsymbol{\theta}}^{*} | {\boldsymbol{\theta}}^{(b)})\)</span> and</p>
<p><span class="math display">\[
A({\boldsymbol{\theta}}^{*}, {\boldsymbol{\theta}}^{(b)}) = \min\left( 1, \frac{L({\boldsymbol{\theta}}^{*}; {\boldsymbol{x}}) f({\boldsymbol{\theta}}^{*})}{L({\boldsymbol{\theta}}^{(b)}; {\boldsymbol{x}}) f({\boldsymbol{\theta}}^{(b)})} \right).
\]</span></p>
</section><section id="utilizing-mcmc-output" class="slide level2">
<h2>Utilizing MCMC Output</h2>
<p>Two common uses of the output from MCMC are as follows:</p>
<ol type="1">
<li><p><span class="math inline">\({\operatorname{E}}[f({\boldsymbol{\theta}}) | {\boldsymbol{x}}]\)</span> is approximated by <span class="math display">\[
\hat{{\operatorname{E}}}[f({\boldsymbol{\theta}}) | {\boldsymbol{x}}] = \frac{1}{B} \sum_{b=1}^B f\left({\boldsymbol{\theta}}^{(b)}\right).
\]</span></p></li>
<li><p>Some subsequence <span class="math inline">\({\boldsymbol{\theta}}^{(b_1)}, {\boldsymbol{\theta}}^{(b_2)}, \ldots, {\boldsymbol{\theta}}^{(b_m)}\)</span> from <span class="math inline">\(\left\{{\boldsymbol{\theta}}^{(b)}\right\}_{b=1}^{B}\)</span> is utilized as an empirical approximation to iid draws from <span class="math inline">\(f({\boldsymbol{\theta}}| {\boldsymbol{x}})\)</span>.</p></li>
</ol>
</section><section id="remarks" class="slide level2">
<h2>Remarks</h2>
<ul>
<li>The random draw <span class="math inline">\({\boldsymbol{\theta}}^{*} \sim q({\boldsymbol{\theta}}| {\boldsymbol{\theta}}^{(b)})\)</span> perturbs the current value <span class="math inline">\({\boldsymbol{\theta}}^{(b)}\)</span> to the next value <span class="math inline">\({\boldsymbol{\theta}}^{(b+1)}\)</span>. It is often a Normal distribution for continuous <span class="math inline">\({\boldsymbol{\theta}}\)</span>.</li>
<li>Choosing the variance of <span class="math inline">\(q({\boldsymbol{\theta}}| {\boldsymbol{\theta}}^{(b)})\)</span> is important as it requires enough variance for the theory to be applicable within a reasonable number of computations, but it cannot be so large that new values of <span class="math inline">\({\boldsymbol{\theta}}^{(b+1)}\)</span> are rarely generated.</li>
<li><span class="math inline">\(A({\boldsymbol{\theta}}^{*}, {\boldsymbol{\theta}}^{(b)})\)</span> is called the acceptance probability.</li>
<li>The algorithm must be run for a certain number of iterations (“burn in”) before observed <span class="math inline">\({\boldsymbol{\theta}}^{(b)}\)</span> can be utilized.</li>
<li>The generated <span class="math inline">\({\boldsymbol{\theta}}^{(b)}\)</span> are typically “thinned” (only sampled every so often) to reduce Markov dependence.</li>
</ul>
</section><section id="full-conditionals" class="slide level2">
<h2>Full Conditionals</h2>
<p>Suppose that <span class="math inline">\({\boldsymbol{\theta}}= (\theta_1, \theta_2, \ldots, \theta_K)\)</span>. Define the subset vector as <span class="math inline">\({\boldsymbol{\theta}}_{a:b} = (\theta_a, \theta_{a+1}, \ldots, \theta_{b-1}, \theta_b)\)</span> for any <span class="math inline">\(1 \leq a \leq b \leq K\)</span>.</p>
<p>The full conditional of <span class="math inline">\(\theta_k\)</span> is</p>
<p><span class="math display">\[
\Pr(\theta_k | {\boldsymbol{\theta}}_{1:k-1}, {\boldsymbol{\theta}}_{k+1:K}, {\boldsymbol{x}})
\]</span></p>
</section><section id="gibbs-sampling" class="slide level2">
<h2>Gibbs Sampling</h2>
<p>Gibbs sampling a special type of Metropolis-Hasting MCMC. The algorithm samples one coordinate of <span class="math inline">\({\boldsymbol{\theta}}\)</span> at a time.</p>
<ol type="1">
<li>Initialize <span class="math inline">\({\boldsymbol{\theta}}^{(0)}\)</span>.</li>
<li>Sample:<br />
<span class="math inline">\(\theta_1^{(b+1)} \sim \Pr(\theta_1 | {\boldsymbol{\theta}}_{2:K}^{(b)}, {\boldsymbol{x}})\)</span><br />
<span class="math inline">\(\theta_2^{(b+1)} \sim \Pr(\theta_2 | \theta_{1}^{(b+1)}, {\boldsymbol{\theta}}_{3:K}^{(b)}, {\boldsymbol{x}})\)</span><br />
<span class="math inline">\(\theta_3^{(b+1)} \sim \Pr(\theta_3 | {\boldsymbol{\theta}}_{1:2}^{(b+1)}, {\boldsymbol{\theta}}_{3:K}^{(b)}, {\boldsymbol{x}})\)</span><br />
<span class="math inline">\(\vdots\)</span><br />
<span class="math inline">\(\theta_K^{(b+1)} \sim \Pr(\theta_K | {\boldsymbol{\theta}}_{1:K-1}^{(b+1)}, {\boldsymbol{x}})\)</span><br />
</li>
<li>Continue for <span class="math inline">\(b = 1, 2, \ldots, B\)</span> iterations.</li>
</ol>
</section><section id="gibbs-and-mh" class="slide level2">
<h2>Gibbs and MH</h2>
<p>As an exercise, show that Gibbs sampling is a special case of the Metropolis-Hastings algorithm where <span class="math inline">\(A({\boldsymbol{\theta}}^{*}, {\boldsymbol{\theta}}^{(b)}) = 1\)</span>.</p>
</section><section id="software" class="slide level2">
<h2>Software</h2>
<p><a href="http://mc-stan.org">Stan</a> is probably the currently most popular software for doing Bayesian computation, including MCMC and variational inference.</p>
<p>There are also popular R packages, such as <a href="https://cran.r-project.org/web/packages/MCMCpack/index.html"><code>MCMCpack</code></a>.</p>
</section></section>
<section><section id="extras" class="titleslide slide level1"><h1>Extras</h1></section><section id="session-information" class="slide level2">
<h2>Session Information</h2>
<section style="font-size: 0.75em;">
<pre class="r"><code>&gt; sessionInfo()
R version 3.6.0 (2019-04-26)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS  10.15.3

Matrix products: default
BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods  
[7] base     

other attached packages:
 [1] forcats_0.5.0   stringr_1.4.0   dplyr_0.8.4    
 [4] purrr_0.3.3     readr_1.3.1     tidyr_1.0.2    
 [7] tibble_2.1.3    ggplot2_3.2.1   tidyverse_1.3.0
[10] knitr_1.28     

loaded via a namespace (and not attached):
 [1] revealjs_0.9     tidyselect_1.0.0 xfun_0.12       
 [4] haven_2.2.0      lattice_0.20-40  colorspace_1.4-1
 [7] vctrs_0.2.3      generics_0.0.2   htmltools_0.4.0 
[10] yaml_2.2.1       rlang_0.4.5      pillar_1.4.3    
[13] withr_2.1.2      glue_1.3.1       DBI_1.1.0       
[16] dbplyr_1.4.2     modelr_0.1.6     readxl_1.3.1    
[19] lifecycle_0.1.0  munsell_0.5.0    gtable_0.3.0    
[22] cellranger_1.1.0 rvest_0.3.5      evaluate_0.14   
[25] labeling_0.3     fansi_0.4.1      broom_0.5.2     
[28] Rcpp_1.0.3       scales_1.1.0     backports_1.1.5 
[31] jsonlite_1.6.1   farver_2.0.3     fs_1.3.1        
[34] hms_0.5.3        digest_0.6.25    stringi_1.4.6   
[37] grid_3.6.0       cli_2.0.2        tools_3.6.0     
[40] magrittr_1.5     lazyeval_0.2.2   crayon_1.3.4    
[43] pkgconfig_2.0.3  xml2_1.2.2       reprex_0.3.0    
[46] lubridate_1.7.4  assertthat_0.2.1 rmarkdown_2.1   
[49] httr_1.4.1       rstudioapi_0.11  R6_2.4.1        
[52] nlme_3.1-144     compiler_3.6.0  </code></pre>
</section>
</section></section>
    </div>
  </div>

  <script src="libs/reveal.js-3.3.0.1/lib/js/head.min.js"></script>
  <script src="libs/reveal.js-3.3.0.1/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display a presentation progress bar
        progress: true,
        // Display the page number of the current slide
        slideNumber: false,
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: true,
        // Transition style
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        // Transition style for full page slide backgrounds
        backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom



        chalkboard: {
        },

        keyboard: {
          67: function() { RevealChalkboard.toggleNotesCanvas() },    // toggle notes canvas when 'c' is pressed
          66: function() { RevealChalkboard.toggleChalkboard() }, // toggle chalkboard when 'b' is pressed
          46: function() { RevealChalkboard.clear() },    // clear chalkboard when 'DEL' is pressed
           8: function() { RevealChalkboard.reset() },    // reset chalkboard data on current slide when 'BACKSPACE' is pressed
          68: function() { RevealChalkboard.download() }, // downlad recorded chalkboard drawing when 'd' is pressed
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'libs/reveal.js-3.3.0.1/plugin/zoom-js/zoom.js', async: true },
          { src: 'libs/reveal.js-3.3.0.1/plugin/chalkboard/chalkboard.js', async: true },
        ]
      });
    </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

<script>
  (function() {
    if (window.jQuery) {
      Reveal.addEventListener( 'slidechanged', function(event) {  
        window.jQuery(event.previousSlide).trigger('hidden');
        window.jQuery(event.currentSlide).trigger('shown');
      });
    }
  })();
</script>


  </body>
</html>
